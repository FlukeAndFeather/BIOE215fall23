[
  {
    "objectID": "reporting_errors.html",
    "href": "reporting_errors.html",
    "title": "Reporting errors",
    "section": "",
    "text": "If you’re struggling with a lesson, exercise, or assessment then there’s a decent chance I made a mistake when I created the materials. Thank you in advance for letting me know! Here’s what I would like you to do.\nThat’s it! I’ll receive an alert that you’ve found an issue. When I fix it, the code change will be linked to your issue.\nHere’s a screencast of me opening an issue and fixing an error that a student discovered."
  },
  {
    "objectID": "reporting_errors.html#why-not-just-email",
    "href": "reporting_errors.html#why-not-just-email",
    "title": "Reporting errors",
    "section": "Why not just email?",
    "text": "Why not just email?\nThis might seem like a lot of work. But I’d like you to try this for a few reasons.\n\nGet more comfortable navigating GitHub repos\nLearn how to use Issues, a great project management tool\nIf/when other students run into the same error, they’ll see that it is in fact an error, which is helpful for everyone’s sanity"
  },
  {
    "objectID": "lessons/02a_quarto_essentials.html",
    "href": "lessons/02a_quarto_essentials.html",
    "title": "Quarto essentials",
    "section": "",
    "text": "Quarto documents combine text, code, and results (e.g., figures and tables). They're a type of literate programming, which differs from scripts because they're intended for a human audience over computation. You'll use Quarto documents for assessments and your final project. Quarto is an extremely powerful data science platform: you can dynamically create PDFs, Word documents, and even websites in Quarto, plus they support R, Python, and other data science programming languages. For now, you just need to know the essentials. Ask yourself the following questions to see if you need this lesson.\nCan you…\nIf you answered yes to all these questions, skip this lesson. Otherwise, choose one of the two options below."
  },
  {
    "objectID": "lessons/02a_quarto_essentials.html#if-youve-used-r-markdown-before",
    "href": "lessons/02a_quarto_essentials.html#if-youve-used-r-markdown-before",
    "title": "Quarto essentials",
    "section": "If you've used R Markdown before…",
    "text": "If you've used R Markdown before…\nIf you've used R Markdown documents before, then you're already familiar with literate programming (though the terminology may be new to you). Quarto is extremely similar to R Markdown, but redesigned for consistency. Read We don't talk about Quarto by Alison Hill to learn what's the same and what's different."
  },
  {
    "objectID": "lessons/02a_quarto_essentials.html#if-youre-brand-new-to-literate-programming",
    "href": "lessons/02a_quarto_essentials.html#if-youre-brand-new-to-literate-programming",
    "title": "Quarto essentials",
    "section": "If you're brand new to literate programming…",
    "text": "If you're brand new to literate programming…\nIf Quarto and R Markdown are unfamiliar to you, I recommend Chapter 29: Quarto, sections 29.1-29.5 (Introduction through Code chunks), in R 4 Data Science by Hadley Wickham. You'll learn the basics of creating Quarto documents, editing in both Visual and Source modes, and adding code chunks."
  },
  {
    "objectID": "lessons/02b_vectors_key_points.html",
    "href": "lessons/02b_vectors_key_points.html",
    "title": "Week 2 - Vector review",
    "section": "",
    "text": "This lesson reviews the key points about vector from lesson Computational Thinking 1."
  },
  {
    "objectID": "lessons/02b_vectors_key_points.html#atomic-vectors",
    "href": "lessons/02b_vectors_key_points.html#atomic-vectors",
    "title": "Week 2 - Vector review",
    "section": "Atomic vectors",
    "text": "Atomic vectors\nJargon:\n\ndata structure. The data you collect might be written on paper or bits collected by an instrument. But R is way too naïve to know anything about that. For R to do something useful with your data (transform it, fit a model to it, make a figure from it), you need to put your data into a data structure. R loves data structures, and if you choose the right data structure for your data then R will make your life way easier.\natomic vector. Just like how atoms are the building blocks of matter, atomic vectors are the building blocks of data in R. They’re the simplest kind of data structure R has. Think of them as variables in data you’d collect in the lab or field, like masses or times or counts. Everything in an atomic vector has to be the same type. Compare against lists (next section).\nelement. Ok, the chemistry analogy breaks down here. Thanks for nothing, R. The individual bits and pieces of data structures are called elements. An atomic vector holding the values 0, -1, and 3.14 has three elements.\n\nQ1: There are four types we care about when it comes to R vectors. What are they?\nQ2: What are the types of each of these vectors? How many elements do they have? What R function tells you the number of elements?\nsci_names &lt;- c(\"Balaenoptera musculus\", \"Balaenoptera physalus\", \"Megaptera noveangliae\")\nquadrat_counts &lt;- c(12L, 0L, 0L, 3L, 0L, 10L)\nrained &lt;- c(TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE)\nreact_time_s &lt;- c(0.13, 0.19, 0.15, 0.10, 0.15, 0.15, 0.15, 0.14)"
  },
  {
    "objectID": "lessons/02b_vectors_key_points.html#lists",
    "href": "lessons/02b_vectors_key_points.html#lists",
    "title": "Week 2 - Vector review",
    "section": "Lists",
    "text": "Lists\nJargon:\n\nlist. If atomic vectors are atoms, then lists are molecules. A list is a more flexible data structure than an atomic vector. They can be made from multiple types of atomic vectors, or even other lists! Data frames (more on those next week) are actually a special form of lists.\n\nHere’s an example of how lists can be built from atomic vectors and/or other lists.\ndog1 &lt;- list(name = \"Bowie\", \n             weight_lb = 80.0, \n             skills = c(\"nap\", \"eat\", \"more nap\"))\ndog2 &lt;- list(name = \"Lassie\",\n             weight_lb = 65.0,\n             skills = c(\"save tommy\", \"bark\", \"acting\"))\ndogs &lt;- list(dog1, dog2)\n\n\n\n\n\n\nNote\n\n\n\nDid you notice the different functions used to make atomic vectors and lists? c() for atomic vectors, list() for lists.\n\n\nQ3: In the code above, what do you think is the length of dog1? dog2? dogs?\nQ4: What’s the type of dog1? dog1 has three elements, called name, weight_lb, and skills. What are their types and lengths?"
  },
  {
    "objectID": "lessons/02b_vectors_key_points.html#atomic-vectors-1",
    "href": "lessons/02b_vectors_key_points.html#atomic-vectors-1",
    "title": "Week 2 - Vector review",
    "section": "Atomic vectors",
    "text": "Atomic vectors\nIf your data are in an atomic vector, then you can index in three different ways to get a subset.\n\nPosition\nLet’s start with indexing by position.\n\n# Start with two vectors: species and migration distances\nspecies &lt;- c(\"Arctic tern\", \"Sooty shearwater\", \"Adelie penguin\")\nmigration_km &lt;- c(96000, 64000, 18000)\n\n# What will these do?\nspecies[1]\nmigration_km[-1]\nmigration_km[1:2]\nspecies[c(1, 1, 1)]\ni &lt;- 3\nspecies[i]\n\nQ5: How would you subset migration_km to get the second element?\nQ6: What are three ways to subset migration_km to get the first and second elements? Hint: use :, c(), and a negative number.\nQ7: How would you use the length() function to get the last element of a vector? Demonstrate with species.\n\n\nName\nSometimes it’s helpful to give the elements of your vectors names. That way you can find the data you want even if you don’t know the position.\n\n# Instead of separate vectors for species and migration distances, we can\n# just name the elements of migration_km themselves.\nmigration_km &lt;- c(ArcticTern = 96000, SootyShearwater = 64000, AdeliePenguin = 18000)\n\n# One element by name\nmigration_km[\"ArcticTern\"]\n\n# Two elements by name (have to use c()! : won't work with names!)\nmigration_km[c(\"ArcticTern\", \"SootyShearwater\")]\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you use c() or list() to make a data structure, follow the pattern c(name = value). R understands the bit to the left of the = is a name from context, so name doesn’t have to go in quotes unless it has a space in it. If you want value to be understood as text, then it would have to go in quotes. So this works c(a = \"b\") and this works c(\"a\" = \"b\") but this doesn’t c(a = b).\n\n\nQ8: Why doesn’t this work? migration_km[ArcticTern]\nQ9: Are migration_km[3] and migration_km[\"AdeliePenguin\"] equivalent? What if I remove an element from migration_km or shuffle it first?\nQ10: Here’s a deliberately tricky question. What does the following give you?\n\nArcticTern &lt;- \"SootyShearwater\"\nmigration_km[ArcticTern]\n\n\n\n\n\n\n\nNote\n\n\n\nYou may have noticed that printing a named vector gives you both the names and the values, which might make it look like there are two vectors. But really it’s just one. The names are basically decorative, displayed on top of the values. When it comes to the data, though, it’s the value that matters.\n\n\nQ11: Below you see the result of subsetting a named vector. What’s the type of the result - character or double?\n\nmigration_km[1]\n\n\n\nLogic\nThe third way to subset a vector is with a bunch of TRUEs and FALSEs. That might seem pointless on its own, but it becomes very useful when you use comparisons like == and &gt; to generate the TRUEs and FALSEs for you.\n\n# Temperature at a site for the first ten days of February\ndates &lt;- paste(\"Feb\", 1:10)\ntemp_F &lt;- c(54.0, 53.8, 53.1, 54, 71.9, 72.0, 54.0, 53.3, 53.9, 53.1)\n\n# Use logic to subset the first and second temperatures\n# This is a silly way to do this\ntemp_F[c(TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)]\n# Slightly less silly, though still silly\ntemp_F[c(rep(TRUE, 2), rep(FALSE, 8))]\n\n# Which days were warmer than the median?\n# This is typically how you'll subset by logic\ndates[temp_F &gt; median(temp_F)]\n\n# Which days were \"heat waves\" (more than 2 standard deviations hotter \n# than the median)\nis_heat_wave &lt;- temp_F &gt; ___ + ___\nheat_wave_dates &lt;- dates[___]\nheat_wave_dates\n\nQ12: Fill in the blanks above to figure out the heat wave dates. What were they?\nQ13: What’s the type of is_heat_wave? Of heat_wave_dates?"
  },
  {
    "objectID": "lessons/02b_vectors_key_points.html#lists-1",
    "href": "lessons/02b_vectors_key_points.html#lists-1",
    "title": "Week 2 - Vector review",
    "section": "Lists",
    "text": "Lists\nLists are more flexible than atomic vectors, so subsetting gets a little more complicated. Subsetting an atomic vector always gives you an atomic vector of the same type, but since lists can hold multiple types they work a little differently.\nThere are three ways to subset lists. [ works similarly as it does with atomic vectors: you get a subset of the list and it’s still a list. [[ and $ are a little different: they pull the contents out of a list. [ might be more familiar, but most of the time [[ and $ do what you actually want them to do. Let’s look at what that means in practice.\n\nSubsetting with [\n\n# Remember our dogs?\ndog1 &lt;- list(name = \"Bowie\", \n             weight_lb = 80.0, \n             skills = c(\"nap\", \"eat\", \"more nap\"))\ndog2 &lt;- list(name = \"Lassie\",\n             weight_lb = 65.0,\n             skills = c(\"save tommy\", \"bark\", \"acting\"))\ndogs &lt;- list(dog1, dog2)\n\n# Subsetting lists with `[` is very similar to atomic vectors, you're \n# always getting a list back\n\n# Index by position\ndog1[1]\ndogs[1]\n\n# Index by name\ndog1[\"weight_lb\"]\ndog2[\"skills\"]\n\n# You can index by logic, too, but it's not terribly helpful here\n\n\n\n\n\n\n\nNote\n\n\n\nWow, lists print a lot differently than atomic vectors, don’t they? That’s the price of being flexible, you end up being more complicated. But they share their roots with atomic vectors. [[1]] means the element at position 1 and $some_name means the element with position some_name. Lists can contain other lists, so [[1]]$some_name means “in the first element, the element named some_name”.\n\n\nQ14: What’s the type of dog1[1]? dog1[1:2]? dogs[2]?\n\n\nSubsetting with [[ and $\n[[ and $ are how you pull the contents out of elements in a list. You can only do this on a single element, so some of the things you can do with [ like x[1:3] and x[c(\"a\", \"b\")] won’t work.\nFor the next examples, you’ll need the palmerpenguins package installed.\n\n# Use `[[` to subset by position\ndog1[[2]]\n\n# Use `[[` to subset by name\ndog1[[\"weight_lb\"]]\n\n# Use `$` to subset by name\ndog1$weight_lb\n\n\n\n\n\n\n\nNote\n\n\n\nDid you notice the difference when subsetting by name with [[ and $? weight_lb had to go in quotes with [[, but was unquoted with $. Subsetting with $ is one of the few times you can use a name unquoted.\n\n\nQ15: Why doesn’t dog1[[weight_lb]] work? Use the error message to explain why.\nQ16: What’s the type of dog1[[2]]? How is it different than dog1[2]?\n\nA minute on data frames\nYou may already be familiar with $ if you’ve used it to subset columns from data frames. Data frames are a special type of list. The columns of the data frame are elements of the list. This kind of building complex data structures from simpler parts is a form of abstraction, which is key to solving problems computationally.\nFor this section, install the palmerpenguins package if necessary.\n\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\n\n# penguins is a data frame, meaning it's a list\ntypeof(penguins)\n\n# The elements of the list are the columns, so the length is the number of columns, not the number of rows.\nlength(penguins)\nnrow(penguins)\nncol(penguins)\n\n# Subsetting a column with $ and [[ returns *just* the column\npenguins$body_mass_g\npenguins[[\"body_mass_g\"]]\ntypeof(penguins$body_mass_g)\n\n# Remember subsetting a list with [ returns a shorter list, so \n# subsetting a column with [ returns a data frame\npenguins[\"body_mass_g\"]\ntypeof(penguins[\"body_mass_g\"])\n\nQ17: length(penguins) giving you the number of columns instead of rows is pretty counterintuitive.Why would columns be the elements in the list instead of the rows? Use types in your answer. Are the types of the elements in columns one type or many? How about rows?\nQ18: If you know the name of the column you want to subset in advance, then $ is convenient e.g., penguins$body_mass_g. But say the name of the column is in a variable e.g., penguin_var &lt;- \"body_mass_g\". How you would you use penguin_var to subset the body_mass_g column?"
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html",
    "href": "lessons/04_comp_thinking_2.html",
    "title": "Week 4 - Computational thinking 2",
    "section": "",
    "text": "No I will not apologize.\nData are nouns. Functions are verbs. Together they make sentences.\nIn Week 2 you learned how R organizes information in data structures. Then in Week 3 you used tidyverse functions to manipulate data frames. Now you’re going to learn how to write your own functions.\n\n\n\nThis is where programming gets fun. Computational thinking is all about breaking problems down into manageable pieces, solving them individually, then re-assembling them into a solution. Functions are both how you solve individual problems and how you reassemble them.\n\n\n\nJust as the crown once gave Prince John a great feeling of power, functions will do the same for you.\n\n\nBut there are other ways to do things, so why should you bother learning functions? You’ve probably written meaningful analyses without ever writing a function yourself, so why take on the learning curve to add functions to your toolbox? Short answer: because they’ll make your life easier. Long answer: scientific analysis in modern eco/evo requires solving complex problems computationally. Without functions, you pretty much have to tackle complex problems as a whole. Our brains are finite, so that limits the complexity of our science. But with functions, we can break down complex problems into simpler components, write functions to solve those components, then solve the whole problem by putting the components back together. Then we can solve vastly more complex problems than what our puny little brains can handle at one time.\nHere’s an example of a complex problem: reading a CSV file. The contents of a CSV file might look like this:\n“city_name”,“population”\n“Santa Barbara, CA”,88255\n“Santa Cruz, CA”,61950\n“Washington, DC”,712816\n“Kennicott, AK”,NA\nThat’s two columns, city_name and population, with four rows. Without functions, reading a CSV file would require you to:\n\nOpen a connection to file.\nRead all the text.\nSplit each row by commas. But only the right commas! E.g. not the one in “Santa Barbara, CA”.\nFigure out the column names.\nFigure out column types, and perhaps convert types as necessary.\nCreate a data frame containing the data.\n\nHere’s how that complex problem works with functions:\n\nCall read_csv()\n\nNow you might be saying, “wait, this is a bogus example because reading a CSV file is a simple task.” In that case, I suggest you try reading a CSV file from scratch in R. You’ll quickly discover that reading CSV files is in fact an extremely complex task. But all that complexity has been rolled up into a function called read_csv(), so you can safely ignore that complexity and focus on your science instead.\nThe fact we can forget how complex a problem like reading CSV files is showcases the the beauty of functions. It’s a principle called abstraction. We take a complex problem, stuff all the logic into a tidy box that’s easy to think about (file path in, data frame out), and free up our cognitive power for other tasks. Today you’ll learn how to do that for your own complex problems."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#lets-put-the-fun-in-functions",
    "href": "lessons/04_comp_thinking_2.html#lets-put-the-fun-in-functions",
    "title": "Week 4 - Computational thinking 2",
    "section": "",
    "text": "No I will not apologize.\nData are nouns. Functions are verbs. Together they make sentences.\nIn Week 2 you learned how R organizes information in data structures. Then in Week 3 you used tidyverse functions to manipulate data frames. Now you’re going to learn how to write your own functions."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#functions-do-things",
    "href": "lessons/04_comp_thinking_2.html#functions-do-things",
    "title": "Week 4 - Computational thinking 2",
    "section": "",
    "text": "This is where programming gets fun. Computational thinking is all about breaking problems down into manageable pieces, solving them individually, then re-assembling them into a solution. Functions are both how you solve individual problems and how you reassemble them.\n\n\n\nJust as the crown once gave Prince John a great feeling of power, functions will do the same for you.\n\n\nBut there are other ways to do things, so why should you bother learning functions? You’ve probably written meaningful analyses without ever writing a function yourself, so why take on the learning curve to add functions to your toolbox? Short answer: because they’ll make your life easier. Long answer: scientific analysis in modern eco/evo requires solving complex problems computationally. Without functions, you pretty much have to tackle complex problems as a whole. Our brains are finite, so that limits the complexity of our science. But with functions, we can break down complex problems into simpler components, write functions to solve those components, then solve the whole problem by putting the components back together. Then we can solve vastly more complex problems than what our puny little brains can handle at one time.\nHere’s an example of a complex problem: reading a CSV file. The contents of a CSV file might look like this:\n“city_name”,“population”\n“Santa Barbara, CA”,88255\n“Santa Cruz, CA”,61950\n“Washington, DC”,712816\n“Kennicott, AK”,NA\nThat’s two columns, city_name and population, with four rows. Without functions, reading a CSV file would require you to:\n\nOpen a connection to file.\nRead all the text.\nSplit each row by commas. But only the right commas! E.g. not the one in “Santa Barbara, CA”.\nFigure out the column names.\nFigure out column types, and perhaps convert types as necessary.\nCreate a data frame containing the data.\n\nHere’s how that complex problem works with functions:\n\nCall read_csv()\n\nNow you might be saying, “wait, this is a bogus example because reading a CSV file is a simple task.” In that case, I suggest you try reading a CSV file from scratch in R. You’ll quickly discover that reading CSV files is in fact an extremely complex task. But all that complexity has been rolled up into a function called read_csv(), so you can safely ignore that complexity and focus on your science instead.\nThe fact we can forget how complex a problem like reading CSV files is showcases the the beauty of functions. It’s a principle called abstraction. We take a complex problem, stuff all the logic into a tidy box that’s easy to think about (file path in, data frame out), and free up our cognitive power for other tasks. Today you’ll learn how to do that for your own complex problems."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#student-learning-objectives",
    "href": "lessons/04_comp_thinking_2.html#student-learning-objectives",
    "title": "Week 4 - Computational thinking 2",
    "section": "Student learning objectives",
    "text": "Student learning objectives\n\nIdentify the parts of a function definition (syntax)\nUse functions to transform inputs (parameters, arguments) into outputs\nWrite a function to automate a common task"
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#identify-the-parts-of-a-function",
    "href": "lessons/04_comp_thinking_2.html#identify-the-parts-of-a-function",
    "title": "Week 4 - Computational thinking 2",
    "section": "Identify the parts of a function",
    "text": "Identify the parts of a function\nJargon\nSyntax. In programming, syntax is basically spelling and grammar. It’s the rules for what the programming language can and cannot understand. For example, you have to use commas to separate arguments in a function call like this c(\"a\", \"b\"). If you leave out the comma c(\"a\" \"b\") then you have a syntax error and R won’t know what to do.\nParameter. The logic inside of a function is unaware of the real world around it, so parameters are placeholders for input.\nArgument. When you call a function on some input, those inputs are the arguments. Think of arguments as real things, which get substituted for the function’s parameters. The distinction between parameters and arguments will become clearer in a bit.\n\nLet’s use a mathematical function as a template for thinking about function syntax, where syntax is the rules for text that R can understand as code. We’ll use the mean, \\(\\bar x\\), defined as:\n\\[\n\\bar x = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\]\nIn plain language, the mean of \\(x\\) is the sum of its elements divided by the number of elements. Before we convert this logic to an R function, let’s break down function syntax.\nAn R function has four parts.\n\nThe name of the function\nThe keyword function\nParameters\nBody\n\nWe can write our own mean() function from those four parts.\nmean &lt;- function(x) {\n  result &lt;- sum(x) / length(x)\n  return(result)\n}\nmean is our function name, followed by the keyword function (a signal to R to expect a function definition), then the parameter(s) in parentheses (which define what input the function expects), and finally the body in curly braces (the function’s actual logic). The last line of the body is a call to return(), which returns the output.\n\n\n\n\n\n\nNote\n\n\n\nreturn() technically isn’t necessary because R will implicitly return whatever the last value in the body is. But I want you to explicitly call it for now because it’s a reminder of what the function’s output is.\n\n\nQ1 I’ve written a function to calculate the standard error and broken it into four parts. Match the parts (left column) to their names (right column).\n\n\n\n\n\n\n\nFunction parts (code)\nFunction parts (names)\n\n\n\n\n{\n  n &lt;- length(x)\n  result &lt;- sd(x) / sqrt(n)\n  return(result)\n}\nFunction name\n\n\nse\nKeyword “function”\n\n\n(x)\nParameters\n\n\nfunction\nBody"
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#transform-arguments-into-output",
    "href": "lessons/04_comp_thinking_2.html#transform-arguments-into-output",
    "title": "Week 4 - Computational thinking 2",
    "section": "Transform arguments into output",
    "text": "Transform arguments into output\nConsider the following code snippet.\nfirst_last_chr &lt;- function(s) {\n  first_chr &lt;- substr(s, 1, 1)\n  last_chr &lt;- substr(s, nchar(s), nchar(s))\n  result &lt;- paste(first_chr, last_chr, sep = \"\")\n  return(result)\n}\ntext &lt;- \"Amazing!\"\nfirst_last_chr(text)\nQ2 What are the four parts of this function?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFunction name = first_last_chr\nKeyword function = function\nParameters = s\nBody = Everything in the {}\n\n\n\nQ3 What output do you get when you call first_last_chr() on text?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA!\n\n\n\nThe distinction between parameters and arguments is subtle, but it’s critical for understanding how functions transform inputs to create outputs.\ns is a parameter. text is an argument. s is a placeholder in the little mini-universe that is the body of first_last_chr(). text is an actual object that we assigned a value to. When we call a function on an argument, the function essentially renames the argument with the name of the parameter.\nLet’s look at the definition of first_last_chr() again.\nfirst_last_chr &lt;- function(s) {\n  first_chr &lt;- substr(s, 1, 1)\n  last_chr &lt;- substr(s, nchar(s), nchar(s))\n  ### PAUSE HERE\n  result &lt;- paste(first_chr, last_chr, sep = \"\")\n  return(result)\n}\nQ4 Let’s say you’re starting with a fresh R session and you run the code above to define first_last_chr. At the line where it says ### PAUSE HERE, what is the value of s?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\ns has no value yet. Parameters don’t have values until the function is called. Then they take on the value of the argument.\n\n\n\nNow call the function on some input.\ntext &lt;- \"Amazing!\"\nfirst_last_chr(text)\nQ5 At the line where it says ### PAUSE HERE, what is the value of s?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBecause we’re in the function body, the parameter s has taken on the value of its argument. So s is \"Amazing!\".\n\n\n\nQ6 Fill in the blank below so the result is “My”.\nfirst_last_chr(\"___\")\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA lot of possible answers. One is first_last_chr(\"Max Czapanskiy\")."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#multiple-parameters",
    "href": "lessons/04_comp_thinking_2.html#multiple-parameters",
    "title": "Week 4 - Computational thinking 2",
    "section": "Multiple parameters",
    "text": "Multiple parameters\nFunctions can have more than one parameter. You’re already familiar with the na.rm parameter in base R’s mean(). Now you’ll add one to your version\nQ7 Fill in the blanks below to add an na.rm parameter to mean(). The skeleton of an if is there to help. If if is new to you, ask a classmate or instructor for help. Test your code with the calls to mean() provided below.\n# A new parameter: na.rm\nmean &lt;- function(x, na.rm) {\n  if (na.rm) {\n    ___\n  }\n  result &lt;- ___\n  return(result)\n}\n\nmean(c(1, 5, 9), na.rm = TRUE)\nmean(c(1, 5, 9), na.rm = FALSE)\nmean(c(1, NA, 5, 9), na.rm = TRUE)\nmean(c(1, NA, 5, 9), na.rm = FALSE)\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmean &lt;- function(x, na.rm) {\n  if (na.rm) {\n    x &lt;- x[!is.na(x)] # or: x &lt;- na.omit(x)\n  }\n  result &lt;- sum(x) / length(x)\n  return(result)\n}\n\n\n\nWhen you start having multiple parameters, some of them may be optional. You can specify a default value for parameters with =. For example, the following function repeats a character string multiple times, with an optional separating character that defaults to _.\nrepeat_chr &lt;- function(s, n, separator = \"_\") {\n  repeated &lt;- rep(s, n) # see ?rep\n  result &lt;- paste(repeated, collapse = separator)\n  return(result)\n}\n\n# Leave `separator` with the default value\nrepeat_chr(\"foo\", 3)\n# Specify the `separator` by name\nrepeat_chr(\"foo\", 3, separator = \" \")\n# Specify the `separator` by position\nrepeat_chr(\"foo\", 3, \" \")\nQ8 Modify mean() so it only removes NAs if you tell it to.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nmean &lt;- function(x, na.rm = FALSE) {\n  if (na.rm) {\n    x &lt;- x[!is.na(x)] # or: x &lt;- na.omit(x)\n  }\n  result &lt;- sum(x) / length(x)\n  return(result)\n}"
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#automating-project-setup",
    "href": "lessons/04_comp_thinking_2.html#automating-project-setup",
    "title": "Week 4 - Computational thinking 2",
    "section": "Automating project setup",
    "text": "Automating project setup\nManually creating a bunch of folders when you create a new project (data/, reports/, docs/, etc) is time consuming and error prone. It’s easy to forget one or misspell a name. Let’s make a function that will automate that for you.\ndir.create() is the function that creates directories. file.create() is the function that creates files. writeLines() will write a character to a file. So the following code creates a Markdown file called bar.md in foo/ with some text in it explaining the purpose of the foo/ directory.\ndir.create(\"foo\")\nfile.create(\"foo/bar.md\")\nwriteLines(\"The foo directory is pointless, except for demonstration\", \n           \"foo/bar.md\")\nCreate a function called project_setup() that doesn’t take any parameters. In the function body, do the following:\n\nCreate the directories for a new project.\nAdd a README.md file to each directory that explains the purpose of that directory.\nReturn the character “SUCCESS!”\n\nNow, let’s use your function to set up your project. Run the code to define your function, then call it. Did it create the directory structure correctly? If not, start debugging! If so, put the function in a script called project_setup.R, and save it to your R/ directory."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#where-did-birds-hatch",
    "href": "lessons/04_comp_thinking_2.html#where-did-birds-hatch",
    "title": "Week 4 - Computational thinking 2",
    "section": "Where did birds hatch?",
    "text": "Where did birds hatch?\nYou’ve learned to use built-in functions, like mean() and min(), with group_by() and summarize() to aggregate data. But sometimes the data aggregation you need to do is more complex or specific to your analysis. In this part of the assessment, you’ll write a custom data aggregation function to use with group_by() and summarize().\nSuddenly you’re a shorebird biologist, analyzing survey data of young-of-the-year Black Oystercatchers in Santa Cruz to figure out where chicks hatched. For safety reasons, the biologists weren’t able to band chicks at their nests. Instead, they caught the chicks later and gave them uniquely identifying 3-color band combinations. For example, GYB is the bird with Green-Yellow-Blue color bands.\nYou know Black Oystercatcher chicks move around, but they tend to stick close to their hatch site. So you’ve decided to estimate the hatching site as the location where the bird was observed most often during weekly surveys.\n\nSimulate data\nFirst, let’s simulate some data to work with.\n\nlibrary(tidyverse)\n\n# Generate sample data\n# Sightings of Black Oystercatcher chicks at Santa Cruz beaches\nbeaches &lt;- c(\"Cowell's\", \"Steamer Lane\", \"Natural Bridges\", \"Mitchell's\", \"Main\")\n# blue, green, black, white, yellow\nband_colors &lt;- c(\"B\", \"G\", \"K\", \"W\", \"Y\") \n# Surveys took place weekly in the summer of 2023\nsurveys &lt;- seq(as.Date(\"2023-06-01\"), as.Date(\"2023-08-31\"), by = 7)\n\n# Setting the \"seed\" forces randomized functions (like sample()) to generate\n# the same output\nset.seed(1538)\n# 3 band colors identify a bird. We want 12 birds.\nbirds &lt;- paste0(\n  sample(band_colors, 25, replace = TRUE),\n  sample(band_colors, 25, replace = TRUE),\n  sample(band_colors, 25, replace = TRUE)\n) %&gt;% \n  unique() %&gt;%\n  head(12)\nbloy_chicks &lt;- tibble(\n  # Randomly generate survey data\n  beach = sample(beaches, size = 100, replace = TRUE),\n  bird = sample(birds, size = 100, replace = TRUE),\n  survey = sample(surveys, size = 100, replace = TRUE)\n) %&gt;% \n  # Remove duplicates (see ?distinct)\n  distinct() %&gt;% \n  # Sort by survey date and location\n  arrange(survey, beach)\n\nQ1 We’re randomly generating data, but we’re all going to end up with the same data frames. How is that happening?\nQ2 Explain in plain language what this part does. Your answer should be one or two sentences\nbirds &lt;- paste0(\n  sample(band_colors, 25, replace = TRUE),\n  sample(band_colors, 25, replace = TRUE),\n  sample(band_colors, 25, replace = TRUE)\n) %&gt;% \n  unique() %&gt;%\n  head(12)\nQ3 We generated 100 random survey observations. How many rows are in bloy_chicks? Why the difference?\n\n\nWithout a custom function\nWe want to estimate where chicks hatched using tidyverse functions. Here’s our process:\n\nFor each bird, where was it seen most often?\nIf multiple sites are tied, choose the one with the earliest observation\nIf still tied, randomly choose one\n\nThe code below consists of three pipelines (sequences of commands linked by pipes). Each pipeline has been shuffled.\n\n# Find most frequent beach per bird\n  group_by(bird) %&gt;% \nbeach_freq &lt;- bloy_chicks %&gt;% \n  count(bird, beach) %&gt;% \n  ungroup()\n  filter(n == max(n)) %&gt;% \n# Find first date for each bird+beach\n  summarize(earliest = min(survey),\nbeach_early &lt;- bloy_chicks %&gt;% \n            .groups = \"drop\")\n  group_by(bird, beach) %&gt;% \n# Join the two conditions and retain most frequent beach, only earliest\n  filter(earliest == min(earliest)) %&gt;% \n  ungroup()\n  sample_n(1) %&gt;% # Randomly choose 1 row. See ?sample_n\n  left_join(beach_early, by = c(\"bird\", \"beach\")) %&gt;% \n  group_by(bird) %&gt;% \nhatch_beach &lt;- beach_freq %&gt;% \n\nQ4 Sort the pipelines back into correct order."
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#with-a-custom-function",
    "href": "lessons/04_comp_thinking_2.html#with-a-custom-function",
    "title": "Week 4 - Computational thinking 2",
    "section": "With a custom function",
    "text": "With a custom function\nThere are two issues with the approach above:\n\nIt’s kind of long and we have to make multiple intermediate data frames. So it’s not the easiest code to read.\nThe logic for estimating a hatching beach is spread out across multiple locations in the code. If we choose a different approach then we have to change everything!\n\nHere’s a different approach using a custom function.\n\nPut the logic for estimating the hatching beach in a single function.\nGroup the data by bird\nSummarize each group using your custom function\n\nThis is an example of a split-apply-combine strategy. Use group_by() to split our data frame by bird. Write a custom function to estimate the hatching beach for that bird. That’s critical: this function works on just one part of the whole! Use summarize() to apply our function to each bird and combine the results.\nBelow is a skeleton of the custom function with key pieces missing, followed by a split-apply-combine skeleton.\n\nfind_hatching_beach &lt;- function(site, date) {\n  # Start with a data frame (or tibble) of site and date for *one* bird\n  # Use pipes and dplyr functions to find the hatching beach\n  bird_observations &lt;- tibble(site, date)\n  result &lt;- bird_observations %&gt;% \n    ___ # use as many pipes and dplyr functions as necessary\n  # result should end up as a data frame with one row for the hatching beach\n  return(result$site) # return the hatching beach\n}\n\n# split-apply-combine\nbloy_chicks %&gt;% \n  group_by(___) %&gt;% \n  summarize(___)\n\nQ5 The two parameters of find_hatching_beach() are named site and date. When this function is called, what columns in bloy_chicks will you use as arguments for these parameters?\nQ6 What will be the value of site when find_hatching_beach() is called on the group for bird YWG? How about WYB?"
  },
  {
    "objectID": "lessons/04_comp_thinking_2.html#bonus-optional-challenge",
    "href": "lessons/04_comp_thinking_2.html#bonus-optional-challenge",
    "title": "Week 4 - Computational thinking 2",
    "section": "Bonus optional challenge",
    "text": "Bonus optional challenge\nThis is a tricky one! I encourage you to work together to figure it out. But it’s totally optional.\nOur workflow with Quarto documents so far has looked like this:\n\nCreate a Quarto document in reports/.\nRender the document, which creates an HTML file and a folder in reports/.\nMove the outputs from reports/ to docs/.\n\nYour challenge: write a function to automate step 3. Call the function move_report_output(). It should have one parameter quarto_doc. In the function body, use dir() to find all the outputs in reports/ created by rendering the document specified by quarto_doc. Move those outputs from reports/ folder to the docs/ folder."
  },
  {
    "objectID": "lessons/05_debugging.html",
    "href": "lessons/05_debugging.html",
    "title": "Week 5 - Troubleshooting",
    "section": "",
    "text": "Use indexing by position, logic, and name to subset data in vectors (review of week 2 material)\nDiagnose bugs by creating a minimum reproducible example\nExplore the state of your R environment using the debugger"
  },
  {
    "objectID": "lessons/05_debugging.html#atomic-vectors-and-lists",
    "href": "lessons/05_debugging.html#atomic-vectors-and-lists",
    "title": "Week 5 - Troubleshooting",
    "section": "Atomic vectors and lists",
    "text": "Atomic vectors and lists\n\nAtomic vectors = basic building blocks. Character, double, integer, and logical.\nLists = complex combinations of atomic vectors and other lists.\n\nTake-home message: just about everything in R is either a list or a function.\nThings that are secretly lists:\n\ndata.frames\nmodels e.g., lm(y ~ x)\nplots e.g., ggplot(dat, aes(x, y)) + geom_point()"
  },
  {
    "objectID": "lessons/05_debugging.html#indexing-by-position-logic-and-name",
    "href": "lessons/05_debugging.html#indexing-by-position-logic-and-name",
    "title": "Week 5 - Troubleshooting",
    "section": "Indexing by position, logic, and name",
    "text": "Indexing by position, logic, and name\nThere are three ways to subset a vector using an index: by position, logic, and name. Use [] to subset a vector.\n\n# A named vector\nx &lt;- c(a = 1, b = 3, c = 5)\n# Index by position\nx[1]\nx[1:2]\nx[c(1, 3)]\n# Index by name\nx[\"a\"]\nx[c(\"a\", \"c\")]\n# Index by logic\nx[x &gt; 3]\nx[names(x) == \"b\"]\n\n\nExercises\nUse the following code to answer questions Q1 - Q3.\n\nplant_height_mm &lt;- c(p0 = 45.94, p1 = 48.13, p2 = 48.14, p3 = 47.55, p4 = 43.85, p5 = 45.12, p6 = 45.49, p7 = 44.82, p8 = 48.4, p9 = 46.62)\nplant_species &lt;- c(\"Arabidopsis thaliana\", \"Arabidopsis arenosa\", \"Arabidopsis lyrata\", \"Arabidopsis arenosa\", \"Arabidopsis arenosa\", \"Arabidopsis arenosa\", \"Arabidopsis lyrata\", \"Arabidopsis thaliana\", \"Arabidopsis thaliana\", \"Arabidopsis thaliana\")\n\nQ1: Match the following lines of code to indexing by position, logic, and name.\n\nplant_height_mm[plant_species == \"Arabidopsis thaliana\"]\nplant_height_mm[c(\"p0\", \"p9\", \"p10\")]\nplant_height_mm[7:10]\n\nQ2: Fill in the blanks to (1) subset plant_height_mm where species is Arabidopsis arenosa and (2) subset plant_height_mm where species isn’t Arabidopsis thaliana.\n\n# (1)\nplant_height_mm[___ == \"___\"]\n# (2)\n___[___ ___ \"Arabidopsis thaliana\"]\n\nQ3: Subset plant_species where the plant height was more than a standard deviation greater than the mean.\nRecall that the columns of data frames are vectors, too. That means indexing and subsetting works with data frame columns. Answer questions Q4 - Q6 about indexing data frame columns.\n\n# install.packages(\"palmerpenguins\") &lt;- run this first if necessary\nlibrary(palmerpenguins)\n\nfemale_bill_length_mm &lt;- penguins$bill_length_mm[!is.na(penguins$sex) & penguins$sex == \"female\"]\n\nQ4: In the code chunk above, what vector are we subsetting? Are we indexing by position, logic, or name?\nQ5: Use the seq() function to subset every 10th element of the island column in penguins. Is this subsetting by position, logic, or name? Hint: use the from, to, and by parameters of seq().\nQ6: Fill in the code chunk below to find the mean of the five largest bill lengths by species (big_bill). Then add another argument to summarize to create lil_bill, which should be the mean of the five smallest bill lengths. Hint for calculating lil_bill: combine length() and : to index by position.\n\npenguins %&gt;% \n  drop_na(bill_length_mm) %&gt;% \n  group_by(species) %&gt;% \n  arrange(desc(bill_length_mm)) %&gt;% \n  summarize(big_bill = mean(___[___]))"
  },
  {
    "objectID": "lessons/05_debugging.html#subsetting-lists",
    "href": "lessons/05_debugging.html#subsetting-lists",
    "title": "Week 5 - Troubleshooting",
    "section": "Subsetting lists",
    "text": "Subsetting lists\nSubsetting lists is a little different than subsetting atomic vectors because you lose the guaranteed type consistency. All elements of an atomic vector are the same type, but a list can contain all sorts of things.\nAs with atomic vectors, you can index lists by position, logic, and name using []. But be careful: [] always returns another list. This is a really common error:\n\n# You start with a list\nl &lt;- list(a = 1:3,\n          b = 4:6)\n# You want the mean of the first element, so you index by position\nmean(l[1])\n\nWarning in mean.default(l[1]): argument is not numeric or logical: returning NA\n\n\n[1] NA\n\n\nAs the warning states, mean() only works on numeric or logical atomic vectors. Even though the first element of l is a numeric atomic vector, l[1] is not an atomic vector. This is because [] always returns another list when subsetting lists. So l[1] is still a list, it just contains the atomic vector 1:3.\nYou can reach inside lists to access their contents using $ (name) or [[]] (name or position).\n\nmean(l$a)\nmean(l[[\"a\"]])\nmean(l[[1]])\n\nNotice that $ doesn’t need you to put the name in quotes but [[]] does. So when you’ve used $ to access columns in data frames, you’re pulling the contents out of a list element by name.\n\n# First ten penguin species\npenguins$speces[1:10]\n\n[[]] is preferable to $ when you need to use a variable to decide what contents you want.\n\nmy_element &lt;- \"a\"\n# This doesn't work\nl$my_element\n# This does\nl[[my_element]]\n\n\nExercises\nUse the code below to answer questions Q7 - Q9 about subsetting and indexing lists.\n\nset.seed(1001)\n# A linear regression model\nl &lt;- list(x = 1:10,\n          y = 2 * x + rnorm(10, sd = 2),\n          coef = c(intercept = 1.326, slope = 1.635))\n\nQ7: What type of indexing is used in the call to plot() to get the x- and y-axis values?\nQ8: Estimate the predicted values of y using coef.\n\nHow will you subset x? [], [[]], or $.\nHow will you subset the intercept and slope? Hint: you’ll need to combine list subsetting and vector subsetting!\nApply the formula \\(y = mx + b\\) to estimate predicted y.\n\nQ9: Create a plot by filling in the blanks in the following code.\n\npredicted_y &lt;- ___           # Your answer to Q8 goes here\nplot(___, ___)               # Create a scatter plot of x and y (from l)\nlines(___, ___, col = \"red\") # Add a line with the model predictions (observed x and predicted y)"
  },
  {
    "objectID": "lessons/05_debugging.html#minimum-reproducible-example",
    "href": "lessons/05_debugging.html#minimum-reproducible-example",
    "title": "Week 5 - Troubleshooting",
    "section": "Minimum reproducible example",
    "text": "Minimum reproducible example\n\nThe small working example Jenny Bryan is talking about here is called a minimum reproducible example, or a reprex for short. Distilling a bug into the smallest amount of data and code that produces the bug is the single most valuable skill for debugging! Creating a reprex will often lead you directly to the source of the bug, and if it falls short of that it will at least narrow things down for other people to help you.\n\n\n\n\n\n\nNote\n\n\n\nThis lesson draws heavily from Jenny Bryan’s 2020 rstudio::conf presentation Object of type ‘closure’ is not subsettable\n\n\nBefore we begin, install and load the reprex package.\n\ninstall.packages(\"reprex\")\nlibrary(reprex)\n\nA reprex has to be two things: reproducible and minimal. The reprex package will ensure your example is reproducible. You, the scientist, have to figure out minimal using your judgement.\n\nReproducible\nreprex() will create a reproducible example of some code for you and format the output so you can share it over GitHub, StackOverflow, etc. Call reprex() on a code chunk wrapped in {}.\n\nreprex({\n  library(palmerpenguins)\n  body_condition &lt;- resid(lm(body_mass_g ~ flipper_length_mm, penguins))\n  summary(body_condition)\n})\n\nreprex will run your code in a fresh R session, then copy the formatted code and outputs to your clipboard. You can paste that directly into a GitHub issue, a Quarto document, or anything else that uses Markdown.\nlibrary(palmerpenguins)\nbody_condition &lt;- resid(lm(flipper_length_mm ~ body_mass_g, penguins))\nsummary(body_condition)\n#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#&gt; -23.7626  -4.9138   0.9891   0.0000   5.1166  16.6392\nCreated on 2023-10-30 by the reprex package (v2.0.1)\nreprex guarantees your code is reproducible by running it in a new session. Try the following:\n\nlibrary(palmerpenguins)\nreprex({\n  body_condition &lt;- resid(lm(body_mass_g ~ flipper_length_mm, penguins))\n  summary(body_condition)\n})\n\nbody_condition &lt;- resid(lm(body_mass_g ~ flipper_length_mm, penguins))\n#&gt; Error in is.data.frame(data): object 'penguins' not found\nsummary(body_condition)\n#&gt; Error in summary(body_condition): object 'body_condition' not found\nCreated on 2023-10-30 by the reprex package (v2.0.1)\nI moved library(palmerpenguins) out of my reprex, so now the output of reprex() has an error message saying object 'penguins' not found. This tells me I’ve left something out and my reprex isn’t reproducible on its own.\n\n\nMinimal\nIf debugging an error is finding a needle in a haystack, then making your reprex minimal shrinks the haystack.\n\nWhen you first encounter a bug, it’ll probably be very complex. It will likely result from loading external data and multiple packages, followed by a lot of code. When generating a reprex, you’re trying to trigger the same bug with:\n\nSmall and simple inputs\nNo extraneous packages\nNo unnecessary function calls\n\nLet’s walk through an example of finding a minimal example. We start with code that triggers a bug. I want to summarize the Palmer penguin data by species and island, with the number of penguins, the mean and standard deviation of the body size, and two different body condition indices.\n\nreprex({\n  # Load some packages\n  library(lubridate)\n  library(nlme)\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Define some functions\n  \n  # Standard error of the mean\n  se_mean &lt;- function(x) {\n    sd(x) / sqrt(length(x))\n  }\n  \n  # Standard error of a proportion\n  se_prop &lt;- function(p, n) {\n    sqrt(p * (1 - p) / n)\n  }\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  # Start manipulating data\n  penguins %&gt;% \n    arrange(body_mass_g) %&gt;% \n    mutate(mass_kg = body_mass_g / 1000) %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(n = sum(!is.na(body_mass_g)),\n              mean_mass_g = mean(body_mass_g),\n              sd_mass_g = sd(body_mass_g),\n              bci_flipper = bci(body_mass_g, flipper_length_mm),\n              bci_bill = bci(body_mass_g, bill_length_mm),\n              .groups = \"drop\")\n})\n\nThere’s no error, but the data frame is the wrong size (I expected one row per species x island combo) and I got a warning. Let’s start paring things back by asking:\n\nAre my inputs small and simple?\nDo I have extraneous packages?\nDo I have unnecessary function calls?\n\nQ10: Identify 1-3 code chunks you think we can eliminate and still encounter the error.\nHere’s a reduced version. We’re only loading essential packages and we removed some unnecessary function calls.\n\nreprex({\n  # Load some packages\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Define some functions\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  # Start manipulating data\n  penguins %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(n = sum(!is.na(body_mass_g)),\n              mean_mass_g = mean(body_mass_g),\n              sd_mass_g = sd(body_mass_g),\n              bci_flipper = bci(body_mass_g, flipper_length_mm),\n              bci_bill = bci(body_mass_g, bill_length_mm),\n              .groups = \"drop\")\n})\n\nThe warning message suggests the bug is happening in summarize(), so let’s start poking around in there by running each argument separately.\n\nreprex({\n  # Load some packages\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  penguins %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(n = sum(!is.na(body_mass_g)))\n})\nreprex({\n  # Load some packages\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  penguins %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(mean_mass_g = mean(body_mass_g))\n})\nreprex({\n  # Load some packages\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  penguins %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(sd_mass_g = sd(body_mass_g))\n})\nreprex({\n  # Load some packages\n  library(palmerpenguins)\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  penguins %&gt;% \n    group_by(species, island) %&gt;% \n    summarize(bci_flipper = bci(body_mass_g, flipper_length_mm))\n})\n\nNow we know we encounter the bug when we call bci() from summarize(). That suggests tidyverse is probably essential, but maybe we can remove the dependency on palmerpenguins if we generate our own data. bci takes two parameters, plus we need a grouping variable, so we’ll make a data frame with three columns.\n\nreprex({\n  # Load a package\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  data.frame(a = rep(\"a\", 10), b = 1:10, c = 1:10) %&gt;% \n    group_by(a) %&gt;% \n    summarize(d = bci(b, c))\n})\n\nThat gets us a lot closer! Maybe we don’t even need the grouping variable?\n\nreprex({\n  # Load a package\n  library(tidyverse)\n  \n  # Body condition index (residual of body mass on structural size e.g. flipper or bill length)\n  bci &lt;- function(mass, structural_size) {\n    resid(lm(mass ~ structural_size))\n  }\n  \n  data.frame(b = 1:10, c = 1:10) %&gt;% \n    summarize(d = bci(b, c))\n})\n\nThis is a pretty minimal reprex now, and ready for us to either diagnose ourselves or share with someone else. We could try to go even more minimal by trying to remove calls to resid() or lm(), but let’s stop here.\nQ11: Do you see the cause of the bug?\nQ12: Compare the minimal reprex to the original code chunk - how does the reprex get you closer to the bug?"
  },
  {
    "objectID": "lessons/05_debugging.html#debugging-with-browser",
    "href": "lessons/05_debugging.html#debugging-with-browser",
    "title": "Week 5 - Troubleshooting",
    "section": "Debugging with browser()",
    "text": "Debugging with browser()\nIt can be difficult to picture what the R environment looks like at the exact moment where a bug occurs. That’s where browser() can help. This function pauses execution, allowing you to look around and explore. Let’s return to the bug in our original code chunk, but add browser() to bci().\n\n# Load some packages\nlibrary(lubridate)\nlibrary(nlme)\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\n# Define some functions\n\n# Standard error of the mean\nse_mean &lt;- function(x) {\n  sd(x) / sqrt(length(x))\n}\n\n# Standard error of a proportion\nse_prop &lt;- function(p, n) {\n  sqrt(p * (1 - p) / n)\n}\n\n# Body condition index (residual of body mass on structural size e.g. flipper or bill length)\nbci &lt;- function(mass, structural_size) {\n  # ADD BROWSER HERE\n  browser()\n  resid(lm(mass ~ structural_size))\n}\n\n# Start manipulating data\npenguins %&gt;% \n  arrange(body_mass_g) %&gt;% \n  mutate(mass_kg = body_mass_g / 1000) %&gt;% \n  group_by(species, island) %&gt;% \n  summarize(n = sum(!is.na(body_mass_g)),\n            mean_mass_g = mean(body_mass_g),\n            sd_mass_g = sd(body_mass_g),\n            bci_flipper = bci(body_mass_g, flipper_length_mm),\n            bci_bill = bci(body_mass_g, bill_length_mm),\n            .groups = \"drop\")\n\nNow when you call the pipeline, browser() will pause R so you can explore. Answer the following questions.\nQ13: Describe the values of bci()’s parameters mass and structural_size the first time browser() is called.\nQ14: What is the value of resid(lm(mass ~ structural_size))?\nQ15: What’s the cause of the bug here?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Your instructor, Max, is a postdoc with NOAA’s Southwest Fisheries Science Center. They began their career as a software engineer at Microsoft then spent ten years as a field ecologist (seabirds, mostly) and ecological data scientist (whales, mostly). His current research is on advancing open science practices in marine ecology. But more than research, their real passion is teaching (specifically, teaching data science to biologists) so they’re grateful for the opportunity to teach Data Science for Eco/Evo this quarter! In his free time, he’s a baker, a runner, and dog drummer (that’s when you drum on the side of your dog as loud as you can).\nYou can check out their personal website at www.flukeandfeather.com."
  },
  {
    "objectID": "about.html#max-czapanskiy-hethey",
    "href": "about.html#max-czapanskiy-hethey",
    "title": "About",
    "section": "",
    "text": "Your instructor, Max, is a postdoc with NOAA’s Southwest Fisheries Science Center. They began their career as a software engineer at Microsoft then spent ten years as a field ecologist (seabirds, mostly) and ecological data scientist (whales, mostly). His current research is on advancing open science practices in marine ecology. But more than research, their real passion is teaching (specifically, teaching data science to biologists) so they’re grateful for the opportunity to teach Data Science for Eco/Evo this quarter! In his free time, he’s a baker, a runner, and dog drummer (that’s when you drum on the side of your dog as loud as you can).\nYou can check out their personal website at www.flukeandfeather.com."
  },
  {
    "objectID": "final_project.html",
    "href": "final_project.html",
    "title": "Final project",
    "section": "",
    "text": "In weeks 1-6 you learned how to set up a computational analysis (using RStudio, git, and GitHub) and techniques for analyzing data (creating and accessing data structures, writing functions, and using the tidyverse). In weeks 7-9, you’ll apply those skills to replicate a key finding (e.g., a figure or table) from a published paper.\nAs science generally becomes more computational, including eco/evo biology, the traditional methods section has become less capable of capturing the complexities of the scientific process. Documenting complex data and code in a manuscript is difficult, and as a result many papers do not adequately document how to replicate their results. Replication studies, such as (Boersch-Supan 2021) in ReScience C, demonstrate the challenges of following the computational methods in published papers. Replication assignments are also effective teaching tools for computational methods and open science practices (Marwick et al. 2019).\nYou’ll work with a group to write your own replication study following these steps:"
  },
  {
    "objectID": "final_project.html#choose-a-study-for-replication",
    "href": "final_project.html#choose-a-study-for-replication",
    "title": "Final project",
    "section": "Choose a study for replication",
    "text": "Choose a study for replication\nAs a group, decide on a published paper for your final project. Identify the key finding you will replicate. The principal requirement for this paper is it must have a data availability statement indicating an open data repository where you can find the data.\n\nSubmission\nDesignate one member of your group to be your repository maintainer. Create a GitHub repository for your project. Add a document to the repository’s website ([your username].github.io/[repository name]) describing the paper and finding you plan to replicate.\nCreate an issue in the repo, add link to the new document, list the group members’ names, and tag me (FlukeAndFeather)."
  },
  {
    "objectID": "final_project.html#identify-the-necessary-metadata-sources",
    "href": "final_project.html#identify-the-necessary-metadata-sources",
    "title": "Final project",
    "section": "Identify the necessary (meta)data source(s)",
    "text": "Identify the necessary (meta)data source(s)\nFind the data you’ll need for your replication, as well as the metadata describing how to reuse those data. Determine whether the (meta)data are sufficient to replicate your key finding.\n\nSubmission\nCreate another document for the repository’s website describing the data and metadata, and how they support the key finding.\nCreate another issue in the repo, link to the new document, and tag me (FlukeAndFeather)."
  },
  {
    "objectID": "final_project.html#create-a-computational-analysis-to-replicate-a-key-finding",
    "href": "final_project.html#create-a-computational-analysis-to-replicate-a-key-finding",
    "title": "Final project",
    "section": "Create a computational analysis to replicate a key finding",
    "text": "Create a computational analysis to replicate a key finding\nThis is the meat of the final project. Write an analysis using the skills you learned during weeks 1-6 to replicate your key finding.\n\nMake a plan\nBefore you write any code, make a written plan. What will your folder/file structure look like? Will you need to preprocess the data? What intermediate output data will you produce? What packages and functions will you need to use?\n\nSubmission\nPut your analysis plan into a document. A flowchart or some other kind of sketch will probably be useful. Add the document to your repository’s website.\nCreate another issue in the repo, link to the new document, and tag me (FlukeAndFeather).\n\n\n\nExecute the plan\nWorking as a team, write your analysis. You will undoubtedly have to deviate from your plan as you learn more about the data and methods - that’s ok!\n\nSubmission\nCreate a document detailing the intermediate steps and final results of your analysis. Add the document to your repository’s website.\nCreate another issue in the repo, link to the new document, and tag me (FlukeAndFeather)."
  },
  {
    "objectID": "final_project.html#write-a-brief-manuscript-documenting-your-process",
    "href": "final_project.html#write-a-brief-manuscript-documenting-your-process",
    "title": "Final project",
    "section": "Write a (brief) manuscript documenting your process",
    "text": "Write a (brief) manuscript documenting your process\nCreate a document called paper.qmd in your reports/ directory for your final manuscript. This manuscript should contain the following.\n\nA brief (~1 paragraph) introduction describing the paper and finding you’re replicating.\nA summary of the data associated with the finding. This should include a bare minimum description of field/laboratory methods (common garden experiment? line transects?) as well as the data’s representation (if it’s a CSV file, what do the columns and rows represent?)\nA summary of the analysis methods. How did you process the data? If you fit a statistical model to the data, how did you do that?\nA description of your results. If you replicated a figure or table, add it here.\nA brief (~2-3 paragraphs) discussion. Compare your results to the original paper. Did you find the same result or were they different? If the original paper’s methods left a step out (e.g., a parameter value), how did you choose what to use?\n\nGeneral guidelines:\n\nUse Quarto’s technical writing features\n\nCross-reference figures and tables using code chunk labels. Don’t write (Figure 1) directly - let Quarto number them for you.\nCite references using a .bib file. The tutorial has good instructions for how to do this.\n\nSave file changes and commit/push early and often!\nUse the scratch/ folder judiciously. As you start exploring the data/analysis, creating an R script in the scratch/ is a good way to start. But as you understand the data/analysis better, make sure to move code out of scratch/ into functions in R/ or standalone scripts (e.g., 00_preprocess_data.R).\n\n\nSubmission\nAdd paper.qmd to your repository’s website. Create your last issue in the repo, link to the paper, and tag me (FlukeAndFeather)."
  },
  {
    "objectID": "discussions/02_open_data_1.html",
    "href": "discussions/02_open_data_1.html",
    "title": "Week 2 - Open data 1",
    "section": "",
    "text": "For this week you read Vines et al. (2014) and Wilkinson et al. (2016) about open data.\nOur discussion today will follow the Snowballing format (Brookfield and Preskill 2016). Here’s how it works.\n\nYou’re presented with two discussion questions.\n\nThink of a time you tried to use data from another scientist, lab, paper, etc. What challenges did you encounter trying to use those data? Conversely, what did the data provider do that made your science easier?\nWhat challenges do you anticipate encountering if/when you share your own data? What would you need to learn or what systems would have to change to make sharing data easier?\n\nTake two minutes to reflect and make a note of your thoughts.\nFind a partner. For three minutes, take turns sharing your thoughts. What did you have in common? What was different? How would you synthesize your two points of view?\nNow your pair finds another pair. For another three minutes, repeat step 3 with the larger group. As the group expands, be mindful of everyone’s participation. A little bit of silence can go a long way to giving everyone a chance to participate.\nRepeat step 4 until the whole group is discussing together.\n\n\n\n\n\nReferences\n\nBrookfield, Stephen D, and Stephen Preskill. 2016. The Discussion Book. London, England: Jossey-Bass.\n\n\nVines, Timothy H., Arianne Y.K. Albert, Rose L. Andrew, Florence Débarre, Dan G. Bock, Michelle T. Franklin, Kimberly J. Gilbert, Jean-Sébastien Moore, Sébastien Renaut, and Diana J. Rennison. 2014. “The Availability of Research Data Declines Rapidly with Article Age.” Current Biology 24 (1): 94–97. https://doi.org/10.1016/j.cub.2013.11.014.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1). https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "discussions/03_open_tools.html",
    "href": "discussions/03_open_tools.html",
    "title": "Week 3 - Open tools",
    "section": "",
    "text": "For this week you read Lowndes et al. (2017) and Marwick, Boettiger, and Mullen (2018) about open tools.\nOur discussion today will follow the Hatful of Quotes format (Brookfield and Preskill 2016). Here’s how it works.\n\nI’ve printed salient quotes from both papers on slips of paper and put them in a “hat”.\nYou’ll randomly draw a quote and take two minutes to think about it on your own. How does the quote relate to the rest of the paper? The other paper? The other content in this course? Your own experiences as a scientist?\nWe hold a group conversation. When it’s your turn to speak, read your quote and share your thoughts. If your quote has already been read, you can affirm, build on, or contradict earlier comments (respectfully).\n\n\n\n\n\nReferences\n\nBrookfield, Stephen D, and Stephen Preskill. 2016. The Discussion Book. London, England: Jossey-Bass.\n\n\nLowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O’Hara, Ning Jiang, and Benjamin S. Halpern. 2017. “Our Path to Better Science in Less Time Using Open Data Science Tools.” Nature Ecology & Evolution 1 (6). https://doi.org/10.1038/s41559-017-0160.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986."
  },
  {
    "objectID": "discussions/04_open_data_2.html",
    "href": "discussions/04_open_data_2.html",
    "title": "Week 4 - Open data 2",
    "section": "",
    "text": "For this week you read Roche et al. (2022). You also submitted a PDF of your candidate replication paper on Canvas.\nIn today’s discussion, you’ll form final project groups of 3-4 students. Share your candidate paper with your group and explain the figure/table you’d like to replicate.\n\n\n\n\n\n\nImportant\n\n\n\nChoose just one figure or table! It should be the key finding of the paper. You don’t need to replicate the whole thing start to finish.\n\n\nWorking together, go through each paper and make a list of data you would need to replicate the figure. Be specific! What tables of data would you need? What columns and rows would need to be in those tables?\nNext week, you’ll use the metrics in Table 1 of Roche et al. (2022) to assess the completeness and reusability of the data in the paper you’ve chosen. The data requirements you come up with today will guide that exercise.\n\n\n\n\nReferences\n\nRoche, Dominique G., Ilias Berberi, Fares Dhane, Félix Lauzon, Sandrine Soeharjono, Roslyn Dakin, and Sandra A. Binning. 2022. “Slow Improvement to the Archiving Quality of Open Datasets Shared by Researchers in Ecology and Evolution.” Proceedings of the Royal Society B: Biological Sciences 289 (1975). https://doi.org/10.1098/rspb.2021.2780."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "",
    "text": "Welcome to Data Science for Eco/Evo! I’m excited to have you in class and hope this course helps you reach your research goals. On this website, you’ll find course logistics, schedule, and lessons. You’ll use GitHub for submitting assessments."
  },
  {
    "objectID": "index.html#course",
    "href": "index.html#course",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Course",
    "text": "Course\nData Science for Eco/Evo\nBIOE215, 3 Credits, Fall 2023"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Instructor",
    "text": "Instructor\nDr. Max Czapanskiy (he/they)\nEmail: mczapans@ucsc.edu"
  },
  {
    "objectID": "index.html#location",
    "href": "index.html#location",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Location",
    "text": "Location\nCoastBio 115"
  },
  {
    "objectID": "index.html#times",
    "href": "index.html#times",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Times",
    "text": "Times\nMondays, 3:30-5:30 pm"
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Office Hours",
    "text": "Office Hours\nTimes: Thursdays 3:30-5:30 pm\nLocation: CoastBio Otter Conference Room\nOr by appointment."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Data Science for Eco/Evo (Fall 23 BIOE215)",
    "section": "Course Description",
    "text": "Course Description\nThis course will aim to be practical and provide structure for learning the computational skills most eco/evo grad students have to pick up on their own. Topics will include programming best practices in R, organizing data and computational projects, and planning for reproducibility. This is a chance to gain knowledge and experience with the nuts and bolts of making your science work in code, without trying to learn generalized linear models at the same time. The course structure will be self-directed for weeks 1-6, followed by a final project in weeks 7-10."
  },
  {
    "objectID": "lessons/06_wrapup.html",
    "href": "lessons/06_wrapup.html",
    "title": "Week 6 - Wrap up skills portion",
    "section": "",
    "text": "Use browser() to diagnose an error in a pipeline\nCreate figures using ggplot\nChoose papers and figures/tables for final project"
  },
  {
    "objectID": "lessons/06_wrapup.html#what-does-browser-do",
    "href": "lessons/06_wrapup.html#what-does-browser-do",
    "title": "Week 6 - Wrap up skills portion",
    "section": "What does browser() do?",
    "text": "What does browser() do?\nWrite down what you remember about browser(). Turn to a partner and compare notes. I will call on pairs to share."
  },
  {
    "objectID": "lessons/06_wrapup.html#error-in-summarize",
    "href": "lessons/06_wrapup.html#error-in-summarize",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Error in summarize()",
    "text": "Error in summarize()\nThe following pipeline has an error. Use it to answer the following questions.\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\ntraits_r2 &lt;- function(trait1, trait2) {\n  summary(lm(trait1 ~ trait2))$rsquared\n}\n\npenguins %&gt;% \n  summarize(flipper_bill_r2 = traits_r2(flipper_length_mm, bill_length_mm))\n\nQ1: How would you describe the error? I’m not asking you describe the cause of the error yet. Describe how the output is different than what you would expect.\nQ2: Where would you add browser() to explore the cause of the error?\nQ3: Does the body of traits_r2 use list or atomic vector indexing? Does it use indexing by position, logic, or name?\nQ4: What’s the cause of the error? How would you fix it?"
  },
  {
    "objectID": "lessons/06_wrapup.html#adding-group_by",
    "href": "lessons/06_wrapup.html#adding-group_by",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Adding group_by()",
    "text": "Adding group_by()\nThe following pipeline is similar to the one above, with an added layer of complexity. Use it to answer the following questions.\n\n# Pipeline 1\npenguins %&gt;% \n  group_by(species) %&gt;% \n  summarize(flipper_bill_r2 = traits_r2(flipper_length_mm, bill_length_mm))\n\n# Pipeline 2\npenguins %&gt;% \n  group_by(species, island) %&gt;% \n  summarize(flipper_bill_r2 = traits_r2(flipper_length_mm, bill_length_mm),\n            .groups = \"drop\")\n\nQ5: How many times does Pipeline 1 call traits_r2()? How about Pipeline 2?\nQ6: Create Pipeline 3 that additionally groups by sex. How many times does Pipeline 3 call traits_r2()?"
  },
  {
    "objectID": "lessons/06_wrapup.html#error-in-group_by-summarize",
    "href": "lessons/06_wrapup.html#error-in-group_by-summarize",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Error in group_by()-summarize()",
    "text": "Error in group_by()-summarize()\nThe following code creates an error in Pipeline 3. Change your Pipeline 3 to use penguins2 instead of penguins, then answer the following questions.\n\nset.seed(12345)\npenguins2 &lt;- penguins %&gt;% \n  drop_na(sex) %&gt;% \n  sample_n(25)\npenguins2[7, 3:6] &lt;- NA\n\nQ7: How would you describe the error?\nQ8: Use browser() to diagnose the error. Hint: c will tell the debugger to continue until the next time it’s called.\nQ9: How would you fix the error?"
  },
  {
    "objectID": "lessons/06_wrapup.html#components-of-ggplot",
    "href": "lessons/06_wrapup.html#components-of-ggplot",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Components of ggplot()",
    "text": "Components of ggplot()\nggplot() has four major components:\n\nData\nScales\nGeometries\nAesthetics\n\n\nData\nA data frame containing the data you want to visualize. Emphasis on data frame. ggplot() is not designed to work with individual vectors, which you might be used to if you’ve used base R, Matlab, or other plotting tools.\n\n\nScales\nScales determine where/how variables in your data will show up in the plot. These include the x- and y-axes, as well as color, size, and more.\n\n\nGeometries\nGeometries determine the geometric properties of your variables. E.g. scatter plots use a point geometry and line plots use a line geometry. Other familiar geometries include histograms and box plots.\n\n\nAesthetics\nThe aesthetics unify data, scales, and geometries. They tell ggplot() how to translate data to scales for each geometry."
  },
  {
    "objectID": "lessons/06_wrapup.html#a-simple-example",
    "href": "lessons/06_wrapup.html#a-simple-example",
    "title": "Week 6 - Wrap up skills portion",
    "section": "A simple example",
    "text": "A simple example\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n\nggplot(data = penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point()\n\n\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\n\n\n\n\n\n\n\nData\nScales\nGeometries\nAesthetics\n\n\n\n\npenguins\nx, y\ngeom_point()\nx = body_mass_g\ny = flipper_length_mm\n\n\nFirst parameter of ggplot()\nImplied by aes()\nAdded to ggplot()\nDefined by aes(), second parameter of ggplot()\n\n\n\nQ10: Change the aesthetics of the plot to show bill_depth_mm on the y-axis.\nQ11: Add an aesthetic for color, so points will be color-coded by species."
  },
  {
    "objectID": "lessons/06_wrapup.html#customizing-scales",
    "href": "lessons/06_wrapup.html#customizing-scales",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Customizing scales",
    "text": "Customizing scales\nIn addition to adding geometries with geom_*(), we can add scales with scale_*_*() to customize how scales appear in the figure. The first * is the name of the aesthetic, and the second * is the type of scale.\n\nggplot(data = penguins) +\n  # You can also define aesthetics directly inside the geometries\n  geom_point(aes(x = body_mass_g, \n                 y = flipper_length_mm,\n                 color = species)) +\n  # x aesthetic, continuous scale\n  scale_x_continuous(\n    # change the axis name\n    name = \"Body mass (g)\",\n    # change the limits\n    limits = c(2000, 8000)\n  ) +\n  # color aesthetic, manual scale\n  scale_color_manual(\n    # set the values for the colors\n    values = c(Adelie = \"cornflowerblue\",\n               Chinstrap = \"firebrick\",\n               Gentoo = \"darkorchid\")\n  )\n\n\n\n\nQ12: What function would you use to customize the y-axis? Use that function to expand the y-axis limits to include 150 and 250.\nQ13: Look up the help for scale_color_brewer(). Change the color scale to use the “Dark2” color palette."
  },
  {
    "objectID": "lessons/06_wrapup.html#other-useful-geometries",
    "href": "lessons/06_wrapup.html#other-useful-geometries",
    "title": "Week 6 - Wrap up skills portion",
    "section": "Other useful geometries",
    "text": "Other useful geometries\nQ14: Use geom_histogram() to visualize the distribution of body masses.\nQ15: Use geom_boxplot() to create boxplots of the flipper lengths by species. It should look like the figure below. Hint: what aesthetics will you use?\n\n\n\n\n\nQ16: geom_pointrange() is like geom_point() with the addition of error bars. Like geom_point(), geom_pointrange() takes x and y aesthetics, but it also takes xmin, xmax, ymin and ymax aesthetics. Create a figure that shows the mean \\(\\pm\\) sd of each penguin species’ body mass and flipper lengths using points and error bars; it should look like the figure below. First, create a summary data frame that calculates the mean and standard deviation of the two variables you need by each species. Then use that summary data frame to create your figure. Hint: geom_pointrange() takes xmin or xmax, same for ymin or ymax. So you’ll need to call geom_pointrange() twice and specify the aesthetics within the geometry."
  },
  {
    "objectID": "lessons/03_working_data.html",
    "href": "lessons/03_working_data.html",
    "title": "Week 3 - Working with data",
    "section": "",
    "text": "What it is\n\nRows = observations, columns = variables, cells = individual observations (Broman and Woo 2018)\nSpreadsheets and relational databases\n\nWhat it isn’t\n\nGeospatial data (vectors and rasters)\nPhylogenetic data (trees)"
  },
  {
    "objectID": "lessons/03_working_data.html#core-principles",
    "href": "lessons/03_working_data.html#core-principles",
    "title": "Week 3 - Working with data",
    "section": "Core principles",
    "text": "Core principles\n\n\n\nFrom R for Data Science (2e) Section 1.1\n\n\n\nUse similar patterns and conventions for every step of the data science workflow\nEmphasis on data frames, the ubiquitous data structure in R for tabular data\nTidy data. Rows = observations, columns = variables, cells = individual observations (Broman and Woo 2018)\n%&gt;% Pipe friendly! More on that shortly."
  },
  {
    "objectID": "lessons/03_working_data.html#load-the-data",
    "href": "lessons/03_working_data.html#load-the-data",
    "title": "Week 3 - Working with data",
    "section": "Load the data",
    "text": "Load the data\nRead and inspect\n\nsurveys &lt;- read_csv(\"data/portal_data_joined.csv\")\nhead(surveys)\nsummary(surveys)\n\nQ1: What’s the type of column species_id? Of hindfoot_length?\nQ2: How many rows and columns are in surveys?"
  },
  {
    "objectID": "lessons/03_working_data.html#selecting-columns-and-filtering-rows",
    "href": "lessons/03_working_data.html#selecting-columns-and-filtering-rows",
    "title": "Week 3 - Working with data",
    "section": "selecting columns and filtering rows",
    "text": "selecting columns and filtering rows\n\n\n\n\n\n\nImportant\n\n\n\nMany tidyverse functions let you treat columns in a data frame as variables. In the following sections, notice we’re not putting column names in quotes.\n\n\nHow do I keep a few columns?\n\nselect(surveys, plot_id, species_id, weight)\nselect(surveys, plot_id, species_id, weight_g = weight)\n\nHow do I remove a few columns?\n\nselect(surveys, -record_id, -species_id)\n\nHow do I keep rows based on a condition?\n\nfilter(surveys, year == 1995)\nfilter(surveys, year == 1995, plot_id == 7)\nfilter(surveys, month == 2 | day == 20)\n\nQ3: filter() surveys to records collected in November where hindfoot_length is greater than 36.0\nQ4: Fix these errors\n\nfilter(surveys, year = 1995)\nfilter(surveys, polt_id == 2)"
  },
  {
    "objectID": "lessons/03_working_data.html#pipes",
    "href": "lessons/03_working_data.html#pipes",
    "title": "Week 3 - Working with data",
    "section": "Pipes %>%",
    "text": "Pipes %&gt;%\nFor a sequence of transformations, use pipes for readability\n\n# Filter then select without pipes, two ways\n# First way\nselect(filter(surveys, year == 1995), plot_id, species_id, weight)\n# Second way\nsurveys_1995 &lt;- filter(surveys, year == 1995)\nsurveys_psw &lt;- select(surveys_1995, plot_id, species_id, weight)\n\nInstead, use pipes! Read them aloud as “then”.\n\nsurveys_psw &lt;- surveys %&gt;% \n  filter(year == 1995) %&gt;% \n  select(plot_id, species_id, weight)\n\nUnder the hood, the left hand side of the pipe becomes the first argument on the right hand side.\nQ5: Use pipes to subset surveys to animals collected before 1995 retaining just the columns year, sex, and weight"
  },
  {
    "objectID": "lessons/03_working_data.html#addchange-columns-with-mutate",
    "href": "lessons/03_working_data.html#addchange-columns-with-mutate",
    "title": "Week 3 - Working with data",
    "section": "Add/change columns with mutate",
    "text": "Add/change columns with mutate\nAdd a column\n\nsurveys %&gt;% \n  mutate(weight_kg = weight / 1000)\n\nAdd multiple columns based on each other\n\nsurveys %&gt;% \n  mutate(weight_kg = weight / 1000,\n         weight_lb = weight_kg * 2.2)\n\nSo many NAs! Add a filter before mutate.\n\nsurveys %&gt;% \n  filter(!is.na(weight)) %&gt;% \n  mutate(weight_kg = weight / 1000,\n         weight_lb = weight_kg * 2.2)\n\nConvert year, month, and day to dates\n\nsurveys %&gt;% \n  select(year, month, day) %&gt;% \n  mutate(date_str = paste(year, month, day, sep = \"-\"),\n         date = as.Date(date_str))\n\nQ6: Create a new data frame from the surveys data that meets the following criteria: contains only the species_id column and a new column called hindfoot_cm containing the hindfoot_length values (currently in mm) converted to centimeters. In this hindfoot_cm column, there are no NAs and all values are less than 3.\nHint: think about how the commands should be ordered to produce this data frame!"
  },
  {
    "objectID": "lessons/03_working_data.html#split-apply-combine-with-summarize",
    "href": "lessons/03_working_data.html#split-apply-combine-with-summarize",
    "title": "Week 3 - Working with data",
    "section": "Split-apply-combine with summarize",
    "text": "Split-apply-combine with summarize\nWhat’s the average weight of the observed animals by sex?\n\nsurveys %&gt;% \n  group_by(sex) %&gt;% \n  summarize(mean_weight = mean(weight, na.rm = TRUE))\n\nGroup by multiple columns, e.g. sex and species.\n\nsurveys %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nNotice the warning message: our output is still grouped by species_id. By default, summarize() only removes one level of grouping. This usually leads to unexpected results. As the warning suggests, use .groups to drop all groups.\n\n\n\nsurveys %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight, na.rm = TRUE),\n            .groups = \"drop\")\n\nNaN is the result of calling mean() on an empty vector. Let’s remove NAs before summarizing.\n\nsurveys %&gt;% \n  filter(!is.na(weight)) %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight),\n            .groups = \"drop\")\n\nCan also generate multiple summaries per group.\n\nsurveys %&gt;% \n  filter(!is.na(weight)) %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight),\n            .groups = \"drop\")"
  },
  {
    "objectID": "lessons/03_working_data.html#sort-with-arrange",
    "href": "lessons/03_working_data.html#sort-with-arrange",
    "title": "Week 3 - Working with data",
    "section": "Sort with arrange",
    "text": "Sort with arrange\n\nsurveys %&gt;% \n  filter(!is.na(weight)) %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight),\n            .groups = \"drop\") %&gt;% \n  arrange(desc(mean_weight))"
  },
  {
    "objectID": "lessons/03_working_data.html#utility-functions",
    "href": "lessons/03_working_data.html#utility-functions",
    "title": "Week 3 - Working with data",
    "section": "Utility functions",
    "text": "Utility functions\ncount(), drop_na()\ncount() is a shortcut to getting the size of groups\n\nsurveys %&gt;% \n  count(sex, species) %&gt;% \n  arrange(species, desc(n))\n\ndrop_na() is a shortcut for removing rows with missing values\n\nsurveys %&gt;% \n  drop_na(weight, sex) %&gt;% \n  group_by(species_id, sex) %&gt;% \n  summarize(mean_weight = mean(weight),\n            min_weight = min(weight),\n            .groups = \"drop\") %&gt;% \n  arrange(desc(mean_weight))\n\nQ7: How many animals were caught in each plot_type surveyed?\nQ8: Use group_by() and summarize() to find the mean, min, and max hindfoot length for each species (using species_id). Also add the number of observations (hint: see ?n).\nQ9: What was the heaviest animal measured in each year? Return the columns year, genus, species_id, and weight."
  },
  {
    "objectID": "lessons/03_working_data.html#joining-data",
    "href": "lessons/03_working_data.html#joining-data",
    "title": "Week 3 - Working with data",
    "section": "Joining data",
    "text": "Joining data\nJoining columns: left_join(), inner_join()\nSay we have more information about some of our taxa.\n\ncount(surveys, taxa)\ntaxa_iucn &lt;- data.frame(\n  taxa = c(\"Bird\", \"Rabbit\", \"Rodent\"),\n  iucn = c(\"NT\", \"LC\", \"LC\")\n)\ntaxa_iucn\n\nLeft join surveys with taxa info by their shared column (the “key”)\n\nsurveys_iucn &lt;- left_join(surveys, taxa_iucn, by = \"taxa\")\nhead(surveys_iucn)\n\nQ10: How many records were there for NT and LC taxa?\nleft_join() keeps everything from the left. So surveys_iucn has NAs for iucn when taxa wasn’t in taxa_iucn.\nQ11: What kind of taxa do the NAs in iucn correspond to?\ninner_join() only keeps records where the key is in both tables.\n\nsurveys_iucn2 &lt;- inner_join(surveys, taxa_iucn, by = \"taxa\")\n\nQ12: How many rows are in surveys_iucn2? What rows are in surveys_iucn that aren’t in surveys_iucn2?"
  },
  {
    "objectID": "lessons/03_working_data.html#wrapping-up",
    "href": "lessons/03_working_data.html#wrapping-up",
    "title": "Week 3 - Working with data",
    "section": "Wrapping up",
    "text": "Wrapping up\nSave all your changes to scratch/lesson3.R. Commit and push."
  },
  {
    "objectID": "lessons/03_working_data.html#download-data",
    "href": "lessons/03_working_data.html#download-data",
    "title": "Week 3 - Working with data",
    "section": "Download data",
    "text": "Download data\nFor this assessment, we’ll use the breeding bird phenology from Hällfors et al. (2020a). Their data are on Dryad (Hällfors et al. 2020b). We’ll complement that with the AVONET (Tobias et al. 2022) database of bird functional traits, which is on Figshare.\nDownload the Dryad dataset. Copy “73_species.csv” and “Traits_73_species.csv” to your data/ folder.\nFrom the AVONET database on Figshare, download “AVONET Supplementary dataset 1.xlsx” do your data/ folder."
  },
  {
    "objectID": "lessons/03_working_data.html#read-data",
    "href": "lessons/03_working_data.html#read-data",
    "title": "Week 3 - Working with data",
    "section": "Read data",
    "text": "Read data\nIn your Quarto document, add a code chunk that loads the tidyverse and reads the data.\n\n\n\n\n\n\nWarning\n\n\n\nQuarto documents interpret file paths relative to the document, not to your project. Since your Quarto document is in reports/, then data/73_species.csv will point to reports/data/73_species.csv, which doesn’t exist. This is where here::here() function comes in useful! Wrap here::here() around a file path and it’ll fix it to be relative to your project, which is what you want.\nThis won’t work:\nread_csv(\"data/73_species.csv\")\nThis will work:\nread_csv(here::here(\"data/73_species.csv\"))\nThis also won’t work (notice it’s inverted):\nhere::here(read_csv(\"data/73_species.csv\"))\n\n\n\nUse read_csv() to read “73_species.csv” and assign it to a variable called bor_nestlings.\nUse read_csv() to read “Traits_73_species.csv” and assign it to a variable called bor_traits.\nUse readxl::read_excel() to read the “AVONET1_BirdLife” sheet from “AVONET Supplementary dataset 1.xlsx” and assign it to a variable called avonet.\n\n\n\n\n\n\n\nTip\n\n\n\nSee ?readxl::read_excel for info on how to read a specific sheet from an Excel workbook."
  },
  {
    "objectID": "lessons/03_working_data.html#explore",
    "href": "lessons/03_working_data.html#explore",
    "title": "Week 3 - Working with data",
    "section": "Explore",
    "text": "Explore\nWe’ll explore the boreal bird nestling data together. Follow along in your Quarto document.\nAre nestlings showing up earlier in the year over time?\n\nall_birds_trend &lt;- bor_nestlings %&gt;% \n  group_by(Year) %&gt;% \n  summarize(mean_doy = mean(Dayofyear))\n\nggplot(all_birds_trend, aes(Year, mean_doy)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\nNote\n\n\n\nThis may be your first time using ggplot. I’ll give you the ggplot code you need for today. You’ll learn more about it next week.\n\n\nThat combines 73 species. Let’s see the breakdown by species.\n\nspecies_trends &lt;- bor_nestlings %&gt;% \n  group_by(Year, Species) %&gt;% \n  summarize(mean_doy = mean(Dayofyear),\n            .groups = \"drop\")\n\nggplot(species_trends, aes(Year, mean_doy, color = Species)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nSo chaotic! What if we just look at the 5 most data-rich species?\n\ndata_richness &lt;- bor_nestlings %&gt;% \n  count(Species)\n\nmost_rich &lt;- data_richness %&gt;% \n  arrange(desc(n)) %&gt;% \n  slice(1:5)\n\nmost_rich_trends &lt;- bor_nestlings %&gt;% \n  filter(Species %in% most_rich$Species) %&gt;% \n  group_by(Species, Year) %&gt;% \n  summarize(mean_doy = mean(Dayofyear), \n            .groups = \"drop\")\n\nggplot(most_rich_trends, aes(Year, mean_doy, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nA general pattern, with one conflicting example. What species have the strongest trends?\n\n# I'm giving you a *function* to help here. You'll learn more about those next week.\n\n# Finds the slope of the relationship between y and x\ntrend &lt;- function(x, y) {\n  xy_lm &lt;- lm(y ~ x)\n  coef(xy_lm)[2]\n}\n\n# Calculate the trend for all species\nbor_trends &lt;- species_trends %&gt;% \n  group_by(Species) %&gt;% \n  summarize(doy_trend = trend(Year, mean_doy))\n\nSpot check two species\n\nsoi &lt;- c(\"ARDCIN\", \"LARMIN\")\nspecies_trends %&gt;% \n  filter(Species %in% soi) %&gt;% \n  ggplot(aes(Year, mean_doy, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lessons/03_working_data.html#your-turn",
    "href": "lessons/03_working_data.html#your-turn",
    "title": "Week 3 - Working with data",
    "section": "Your turn",
    "text": "Your turn\nFind the species with the most positive and most negative trends, then plot just those.\n\nnrow_bor_trends &lt;- nrow(bor_trends) # Use this later\nbor_extreme &lt;- bor_trends %&gt;% \n  # Sort by the day of year trend\n  ___(___) %&gt;% \n  # Keep just the first (most negative trend) and last (most positive trend) rows\n  slice(c(___, ___))\n\n# Now plot them\nspecies_trends %&gt;% \n  filter(Species %in% ___) %&gt;% \n  ggplot(aes(Year, mean_doy, color = Species)) + \n    geom_point() +\n    geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "lessons/03_working_data.html#bring-in-more-data",
    "href": "lessons/03_working_data.html#bring-in-more-data",
    "title": "Week 3 - Working with data",
    "section": "Bring in more data",
    "text": "Bring in more data\nNext we’re going to test the hypothesis that smaller birds have more flexible phenology, i.e. the absolute value of the trend is correlated with smaller body sizes.\nTo test our hypothesis, we need to add AVONET data to our phenology data by joining. The keys are a bit tricky here!\nbor_trends has a column called Species with a 6-letter code.\nbor_traits has a column called Abbreviation with the 6-letter code and a column called `Scientific name` with the binomial name.\navonet has column called Species1 with the binomial name.\nWe need to join bor_trends to bor_traits, then join with avonet.\nWelcome to data in the real world!\n\n# First, select and rename important columns\navonet_size &lt;- select(avonet, SciName = Species1, Mass_g = Mass)\nbor_sciname &lt;- select(bor_traits, \n                      Species = Abbreviation, \n                      SciName = `Scientific name`)\n\nNow join them all together.\n\nbor_trends_size &lt;- bor_trends %&gt;% \n  left_join(___, by = ___) %&gt;% \n  left_join(___, by = ___) %&gt;% \n  mutate(abs_trend = abs(doy_trend))\n\n# Plot it\nggplot(bor_trends_size, aes(Mass_g, abs_trend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nThat’s kind of hard to interpret. Make a new variable, bor_trends_size2, that removes the missing values in Mass_g and keeps only birds smaller than 2000 g with an absolute trend less than 1.\n\nbor_trends_size2 &lt;- ???\n  \nggplot(bor_trends_size2, aes(Mass_g, abs_trend)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nIs our hypothesis supported?"
  },
  {
    "objectID": "lessons/03_working_data.html#submission",
    "href": "lessons/03_working_data.html#submission",
    "title": "Week 3 - Working with data",
    "section": "Submission",
    "text": "Submission\n\nRender index.qmd, which produces a few outputs in your reports/ folder.\nMove your output (an HTML file and a folder) from reports/ to docs/.\nEnable GitHub pages for your repo.\nCommit your changes and push.\nOpen an issue and tag me in it."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html",
    "href": "lessons/02_comp_thinking_1.html",
    "title": "Week 2 - Computational thinking 1",
    "section": "",
    "text": "Today’s topic is computational thinking. The particular computational thinking skills we’re focusing on are data structures and abstraction. Abstraction is about simplifying problems - it’s how you take a complex, unique system and decompose it into patterns that are easier to think about and work with. It’s arguably the single most important skill for doing things with code. But before we can think about a problem abstractly, we first need patterns to match our problem to. That’s where data structures come in: they’re how we organize and store data in code.\nDepending on your previous experience level, today’s lesson may feel like you’re revisiting material you’ve already learned. But when it comes to programming, most biologists learn to run before they can walk. The purpose of today’s lesson is to give you a more comprehensive understanding of what’s happening when you use R’s built-in data structures.\nWe’re going to begin with a pre-assessment. Don’t worry if some of the questions feel unfamiliar - that’s expected and normal! Based on the pre-assessment, I’m going to recommend one of two different lessons for you."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#which-lesson-should-you-choose",
    "href": "lessons/02_comp_thinking_1.html#which-lesson-should-you-choose",
    "title": "Week 2 - Computational thinking 1",
    "section": "Which lesson should you choose?",
    "text": "Which lesson should you choose?\n\nWriting R code from scratch (team Porifera)\nYou can run code someone else wrote or you can look up code online and paste that into your script, but starting your own script from scratch is a bit mystifying. This lesson introduces the fundamentals of working in R, like creating scripts and making variables. It also covers the fundamental data structure in R: vectors. You’ll learn what vectors are, their different flavors, how to create/subset them, and how to call functions on them.\n\n\nWhy did you do it that way? (team Ctenophora)\nYou’ve memorized recipes and formulas for getting R to do what you want, but maybe you don’t know why things work they do. Usually variables behave the way you expect. However when things go sideways you don’t know where to begin diagnosing the issue. This lesson will formalize your understanding of types and how the more complex R data structures (like lists and data frames) are composed from simpler structures (like atomic vectors of numbers or text)."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#team-porifera",
    "href": "lessons/02_comp_thinking_1.html#team-porifera",
    "title": "Week 2 - Computational thinking 1",
    "section": "Team Porifera",
    "text": "Team Porifera\nThis lesson is from the Data Carpentry Semester Biology Course.\n\nReading\nComplete the Introduction to R lesson from Data Analysis and Vizualisation in R for Ecologists.\n\n\nExercises\nBasic expressions\nBasic variables\nMore variables\nBuilt-in functions\nModify the code\nBasic vectors\nNulls in vectors\nShrub volume vectors\nVariable names"
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#team-ctenophora",
    "href": "lessons/02_comp_thinking_1.html#team-ctenophora",
    "title": "Week 2 - Computational thinking 1",
    "section": "Team Ctenophora",
    "text": "Team Ctenophora\n\nReading\nRead sections 3-3.6.2 in Chapter 3: Vectors and sections 4-4.2.4, and 4.3-4.3.2 in Chapter 4: Subsetting in Advanced R by Hadley Wickham.\n\n\nExercises\nUse the exercise questions at the end of each section in the chapter."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#quarto",
    "href": "lessons/02_comp_thinking_1.html#quarto",
    "title": "Week 2 - Computational thinking 1",
    "section": "Quarto",
    "text": "Quarto\nThis assessment involves a Quarto document. If you’re already familiar with Quarto, move on to the next section. Otherwise, complete the Quarto Essentials mini-lesson."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#set-up",
    "href": "lessons/02_comp_thinking_1.html#set-up",
    "title": "Week 2 - Computational thinking 1",
    "section": "Set up",
    "text": "Set up\n\nCreate a new RStudio project with a git repository called compthinking1. Create a GitHub repository to go with it.\nCreate a folder structure in your project to match what you learned in Week 1.\nIn your reports/ directory, create a Quarto document called index.qmd."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#questions",
    "href": "lessons/02_comp_thinking_1.html#questions",
    "title": "Week 2 - Computational thinking 1",
    "section": "Questions",
    "text": "Questions\nAbstraction is a key computational thinking skill: filtering out all but the key details and matching your task to the right patterns. When it comes to data, abstraction means learning how to represent complex real-world data in standard data structures. In today’s lesson you learned about the fundamental data structure in R: vectors.\nAnswer the following questions in your Quarto document. Choose the questions according to the lesson you completed. Pretend this assessment is a report about your research you’re sharing with a collaborator: add descriptive text and use Markdown formatting to make it easy to read.\n\nTeam Porifera\nQ1\nMatch the following types of real-world data to their equivalent R type and explain your reasoning.\n\nReal world data: scientific names, the count of plants in a quadrat, whether or not it rained on a sequence of days, and the reaction times of birds to disturbances in seconds\nR types: logical, integer, numeric, and character\n\nQ2\nUse this code chunk to answer the following questions.\nbird_mass_g &lt;- c(100.1, 99.2, 99.3, NA, 100.0, 101.5, 94.7, 99.2, 108.2)\nmean_mass &lt;- mean(bird_mass_g)\nsd_mass &lt;- sd(bird_mass_g)\nis_outlier &lt;- bird_mass_g &gt; mean_mass + 3 * sd_mass\nnum_outliers &lt;- sum(is_outlier)\n\nIn plain english, what does this code chunk do?\nTwo of the lines have mistakes that keep the code from doing what it’s supposed to. What are the mistakes and how would you fix them?\nWhat are the types of bird_mass_g, is_outlier, and num_outliers?\n\nQ3\nHow could you change the first line of code in the following chunk so that median_count comes out to 5L?\nquad_counts &lt;- c(2L, 19L, 4L, 5L, 5L, 12L, 0L, 7L)\nvalid_quads &lt;- c(1, 2, 3, 5, 7)\nquad_counts2 &lt;- quad_counts[valid_quads]\nmedian_count &lt;- median(quad_counts2)\n\n\nTeam Ctenophora\nQ1\nLookup tables are a common use case for named vectors. Let’s say you’re surveying the avian community of a wetland. In your notebook, you recorded species by their 4-letter code: GREG for Great Egret, MALL for Mallard, MAWR for Marsh Wren, and KILL for Killdeer. You need a table of counts, but you need the table to show the full common name, not just the 4-letter code you used for convenience. Modify the code below to accomplish that task. First, make species_code a named vector, then modify the call to table() to use species_code as a lookup table that converts 4-letter codes to common names.\nspecies_codes &lt;- c(  \n    # Fill this in\n)\n\nsightings = c(\"GREG\", \"GREG\", \"MALL\", \"MAWR\", \"KILL\", \"GREG\")\n\n# Modify this line to use species_codes so your counts have common names\ntable(sightings)\nQ2\nThese questions are about a built-in R class that is an extremely common source of confusion: POSIXct.\n\nExplain why adding 1 to a POSIXct datetime object increments it by a second. What type is POSIXct (type, not class)? How does POSIXct use that type to represent datetimes?\nWhat’s the default origin of POSIXct objects? Write a line of code that converts the number 0 to January 1, 2000 at noon in UTC using as.POSIXct() (hint: look up the documentation for the origin parameter in the as.POSIXct() help page).\n\nTake away the POSIXct class from the object you just made. What kind of vector is it now? Why isn’t the value 0 anymore?\n\n\nQ3\nWhat are the types of y and z in the following code? Why are they different?\nx &lt;- list(1, 2, 3)\ny &lt;- x[2]\nz &lt;- x[[2]]\nQ4\nThese questions are about the data structure underlying data frames.\n\nYou learned that data frames are built on top of lists. In a data frame, what are the elements of the list - rows or columns?\nWhen called on a data frame, is length() equivalent to nrow() or ncol()? Why?\n\nQ5\nSay you have four experimental plots that you’re supplementing with fertilizer. In the code below, which subsetting operator ($, [, or [[) would you use to pull out the column specified by the variable nutrient? Does your answer change if I specify the result has to be an atomic vector? Why or why not?\nexperiment &lt;- data.frame(\n    plot = c(\"p1\", \"p2\", \"p3\", \"p4\"),\n    N_g = c(0.2, 0.7, 0.3, 0.2),\n    P_g = c(0.1, 0.1, 0.5, 0.6)\n)\n\nnutrient &lt;- \"N_g\"\nQ6\nThe elements of lists can be anything, even other lists. That’s why a common use case for lists is splitting data frames into groups (note: if you’re familiar with dplyr::group_by(), just know this is something different). Explain what the following code does in plain language. If you want to play around with the code (and I recommend you do!) you may need to install the palmerpenguins package. Use ?split for more information about that function.\nlibrary(palmerpenguins) \npenguins_by_island &lt;- split(penguins, penguins$island)\nmean_mass &lt;- list(\n    Biscoe = mean(penguins_by_island$Biscoe$body_mass_g, na.rm = TRUE),\n    Dream = mean(penguins_by_island$Dream$body_mass_g, na.rm = TRUE),\n    Torgersen = mean(penguins_by_island$Torgersen$body_mass_g, na.rm = TRUE)\n)"
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#submission",
    "href": "lessons/02_comp_thinking_1.html#submission",
    "title": "Week 2 - Computational thinking 1",
    "section": "Submission",
    "text": "Submission\n\nRender reports/index.qmd, which creates reports/index.html. Move reports/index.html to docs/.\nActivate your GitHub repository’s Pages. What do you have to do to make docs/index.html render on your Pages site?\nOpen an Issue in your repository on GitHub. Provide a link to your repo’s Pages site. It should look like yourusername.github.io/compthinking1. Tag me in your issue (my user name on GitHub is FlukeAndFeather). In the issue’s thread, tell me how you updated your computational project organization notes during this assessment..\n\nNote: I’m deliberately not providing instructions for creating GitHub Issues. I want you to rely on experimentation, the internet, and your peers. If you haven’t opened an issue before, try the following in this order:\n\nTry on your own\nAsk Google\nAsk a colleague (i.e., your classmates)\n\nCongrats! You’re done with Week 2! Keep up the good work!"
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#types",
    "href": "lessons/02_comp_thinking_1.html#types",
    "title": "Week 2 - Computational thinking 1",
    "section": "Types",
    "text": "Types\n\nR has four important atomic vectors. These vectors hold data of the same type: logical (TRUE, FALSE), integer (1L, 2L), double (3.14, 100.0), and character (\"a\", \"Homo sapiens\").\nR holds mixed-type data in lists. For example, dog &lt;- list(name = \"Bowie\", weight_lb = 80.0) creates a list that describes my dog with both character and double values. Data frames are derived from lists."
  },
  {
    "objectID": "lessons/02_comp_thinking_1.html#indexing",
    "href": "lessons/02_comp_thinking_1.html#indexing",
    "title": "Week 2 - Computational thinking 1",
    "section": "Indexing",
    "text": "Indexing\nYou access data in vectors using indexing.\n\nAtomic vectors\n\nIndex into atomic vectors with the [ operator.\nThere are three ways to index atomic vectors: position, logic, and name.\n\n\n# Create a vector with names\nbounding_box &lt;- c(xmin = -122, xmax = -118, ymin = 33, ymax = 38)\n\n# Indexing by position:\n# One element\nbounding_box[1]\n\nxmin \n-122 \n\n# Two elements with :\nbounding_box[1:2]\n\nxmin xmax \n-122 -118 \n\n# Two elements with c()\nbounding_box[c(1, 4)]\n\nxmin ymax \n-122   38 \n\n# All but one element with negative\nbounding_box[-1]\n\nxmax ymin ymax \n-118   33   38 \n\n# Indexing by logic\nbounding_box[c(TRUE, FALSE, FALSE, FALSE)]\n\nxmin \n-122 \n\n# Usually create logic vector with comparison operator\nis_negative &lt;- bounding_box &lt; 0\nbounding_box[is_negative]\n\nxmin xmax \n-122 -118 \n\n# Same as above, but in one line\nbounding_box[bounding_box &lt; 0]\n\nxmin xmax \n-122 -118 \n\n# Indexing by name\n# One element\nbounding_box[\"xmin\"]\n\nxmin \n-122 \n\n# Two elements with c()\nbounding_box[c(\"xmin\", \"xmax\")]\n\nxmin xmax \n-122 -118 \n\n\n\n\nLists\n\nThere are three operators for indexing into lists: [, [[, and $.\n[ subsets the list, yielding a shorter list. As with atomic vector indexing, can use position, logic, or name.\n[[ and $ pull contents themselves out of the list. Only allows access from one element, by position or name.\n\n\n# Create a list with names\ndog &lt;- list(name = \"Bowie\", \n            weight_lb = 80.0, \n            skills = c(\"nap\", \"eat\", \"more nap\"))\n\n# Subset the list by position...\ndog[2]\n\n$weight_lb\n[1] 80\n\n# ... by logic ...\ndog[c(FALSE, TRUE, FALSE)]\n\n$weight_lb\n[1] 80\n\n# ... or by name\ndog[\"weight_lb\"]\n\n$weight_lb\n[1] 80\n\n# Pull contents from the list by position with [[ ...\ndog[[2]]\n\n[1] 80\n\n# ... by name with [[ ...\ndog[[\"weight_lb\"]]\n\n[1] 80\n\n# ... or by name with $\ndog$weight_lb\n\n[1] 80"
  },
  {
    "objectID": "lessons/01_comp_proj_org.html",
    "href": "lessons/01_comp_proj_org.html",
    "title": "Week 1 - Computational project organization",
    "section": "",
    "text": "Students will be able to:\n\nDescribe the relationship between RStudio project, git repo, and GitHub repo\nOrganize files and folders to maximize reproducibility and collaboration\nCreate a project website on GitHub"
  },
  {
    "objectID": "lessons/01_comp_proj_org.html#student-learning-objectives",
    "href": "lessons/01_comp_proj_org.html#student-learning-objectives",
    "title": "Week 1 - Computational project organization",
    "section": "",
    "text": "Students will be able to:\n\nDescribe the relationship between RStudio project, git repo, and GitHub repo\nOrganize files and folders to maximize reproducibility and collaboration\nCreate a project website on GitHub"
  },
  {
    "objectID": "lessons/01_comp_proj_org.html#todays-lesson",
    "href": "lessons/01_comp_proj_org.html#todays-lesson",
    "title": "Week 1 - Computational project organization",
    "section": "Today’s lesson",
    "text": "Today’s lesson\ntl;dr Each of your analyses should have a standalone directory with all the code and data necessary to produce your results.\nThis lesson has readings, an exercise, and an assessment. Feel free to jump around and do them in whatever order works best for you. I suggest the following order:\n\nRead Bryan (2017) about project-oriented workflows\nIf you’re new to git, read Bryan (2018) about version control and scientific analysis. Consider yourself familiar with git if you know what commit, push, and pull do. If that’s the case, read Braga et al. (2023) about using GitHub for managing scientific projects.\nComplete the Exercise\nComplete the Assessment\n\nBut first, watch this Tik Tok\n\n\nReadings\nThe readings will help clarify why the skills you’re learning will improve your productivity and make your science easier to share, reproduce, and collaborate on.\n\nProject workflows\n\n(Bryan 2017)\nOptionally: chapter 3 of (British Ecological Society and Cooper 2018) Organising projects for reproducibility\n\n\n\nVersion Control\n\n(Bryan 2018) if you’re new to version control\n(Braga et al. 2023) if you already have some experience\n\n\n\n\nExercise\nIn this exercise you’re going to set up an analysis to take advantage of project organization tools in RStudio, git, and GitHub. We’ll use this framework for exercises and assessments throughout the rest of the course.\n\nGoals\n\nCreate an RStudio project and git repository on your computer\nCreate a corresponding GitHub repository online\nSet up an analysis-friendly folder structure\nActivate GitHub pages to give your project a website\n\n\n\nStep 1: Create an RStudio project and git repository\nOpen RStudio. From the top menu, click on File &gt; New Project to launch the New Project Wizard.\nChoose New Directory &gt; New Project. You should now see the Create New Project prompt.\n\nThe Directory name is the name of your project. Call it “bioe215lesson1”. When it comes to project directory naming, there are a couple of best practices to follow. Stick to letters and numbers, and avoid special characters like spaces, underscores, and dashes. This ensures your directory name will be compatible with the naming requirements for R, git, and GitHub.\nNext you’ll choose where to put your project on your computer. In the screenshot above, Create project as subdirectory of: is set to /Users/frank/Documents/GitHub, which is where I keep projects on my computer. Yours may default to somewhere unhelpful. I suggest creating a subfolder in your “Documents” directory called “GitHub”, then use the “Browse…” button to navigate there.\nMake sure the check box for Create a git repository is checked.\nClick on Create Project.\n\nOnce RStudio finishes creating your project, you should see something like the screenshot above. Make sure you see both the git pane and the project name. Let the instructor know when you’re done with this step so they can check everything looks right.\n\n\nStep 2: Create a GitHub repository\nRStudio projects and git repos both live locally on your computer. GitHub repos are remote repositories on the internet. Now you’re going to create a remote GitHub repository.\nFirst, install the usethis package, which has helpful functions for project organization and management. At the R console, run install.packages(\"usethis\").\nIf you don’t have a GitHub account yet, now’s the time to create one. Go to github.com and create an account.\nMake sure git is configured on your machine to have the correct user name and email. In the Terminal pane (not the Console pane)1, run git config --list. Your user.name should be your name and user.email should be the same email you used with your GitHub account. If they’re not configured correctly, run the following R command in the Console. Change YOURNAME and YOUREMAIL accordingly. This is a one-time setup command, you won’t have to do it again until you replace your computer.\nusethis::use_git_config(user.name = \"YOURNAME\", user.email = \"YOUREMAIL\")\n[Is the double colon :: unfamiliar? This operator tells R to look for a function in a certain package. So this command uses the use_git_config() function from the usethis package. Alternatively, you can call library(usethis) and then call the function directly.]\nThe last step before you can connect RStudio to GitHub is saving your credentials. The safest way to do that is with Personal Access Tokens, or PATs. These tell GitHub you are who you say you are and prevent anyone else from messing with your remote repositories2. If you know you already have a PAT, continue to the next step. This is easy to do with usethis. Call usethis::create_github_token(), which will launch GitHub in your web browser. Give your token the name “RSTUDIO” and change the Expiration from 30 days to 90 days. Scroll all the way to the bottom and click the green Generate token button.\n\nYou should see a long line of text with a green check mark next to it. That’s your token. Copy it, then go back to RStudio. At the Console, run gitcreds::gitcreds_set(). You’ll get a prompt to ? Enter new password or token:. Paste your token and click enter. Now RStudio and GitHub can talk to each other3.\nAll that’s left to do now is creating your GitHub repo. usethis helps automate this step. First, call usethis::use_git() and commit the uncommitted files. Then call usethis::git_default_branch_rename()4. Finally, call usethis::use_github(). You should now see your GitHub repository in your browser. Once again, let the instructor know when you’re done with this step so they can check everything looks right.\n\n\nStep 3: Set up folder structure\nThere are a lot of ways you can organize a computational project. It’s less important which system you use than it is to use a system consistently. Consistency reduces headaches! In this exercise, you’re going to use a modified version of the system described by Annna Krystalli in the Organising projects for reproducibility chapter of British Ecological Society and Cooper (2018) (some of the following comes from there verbatim). Create the following folders in your project.\n\nThe data folder contains all input data (and metadata) used in the analysis.\nThe paper folder contains the manuscript.\nThe figs directory contains figures generated by the analysis.\nThe output folder contains any type of intermediate or output files (e.g. simulation outputs, models, processed datasets, etc.). You might separate this and also have a cleaned-data folder.\nThe R directory contains R scripts with function definitions.\nThe reports folder contains Quarto documents5 that describe the analysis or report on results.\nThe docs folder contains the rendered versions of the reports.\nThe scratch folder contains early prototypes and other code I don’t fully understand yet.\nThe scripts that actually do things are stored in the root directory, but if your project has many scripts, you might want to organize them in a directory of their own.\n\nHere’s an example of what your project could look like when you’re done. Notice the numbered scripts: these run the steps of your analysis in order. The rest of the folders hold the components of your analysis so they’re easy to find.\n\nThe biggest benefit of adopting this system is the cognitive space it frees up in your brain. Any brainpower you were devoting to figuring out where to put a file or where to find something can now be reallocated to your actual science.\nA quick note about data. If your raw data are small (&lt;100 MB) and you have permission to make them public6, then it’s ok to store them on GitHub. If they’re big or private, you’ll need to keep them off GitHub. That’s what the .gitignore file is for. If necessary, call usethis::use_git_ignore(\"data/*\"), which will add your data/ directory to .gitignore and keep it out of GitHub.\n\n\nStep 4: Activate GitHub pages\nGitHub has an option for creating a website out of your repository. This is an incredible feature for working with collaborators! You can put your methods and results in Quarto documents that GitHub serves for you. Compared to email threads, a project website does a much better job keeping co-authors in the loop and getting new collaborators up to speed. The folder organization system you created in Step 3 makes this pretty simple.\nLet’s start with create a simple README. GitHub will turn everything in your docs/ directory into the project website. By default, anything called “index” will be your landing page. Create a text file called docs/index.md. The “.md” suffix stands for Markdown. You’ll learn more about Markdown next week. For now, it’s enough to know Markdown is a text-only format that allows basic formatting. When you open index.md, you’ll see RStudio’s visual editor. At the top of the editor, switch from Visual to Source. Add the following text.\n# README\n\nThis is my project website for the Computational Project Organization lesson of *Data Science for Eco/Evo*.\n\nThe reading assessment answers are [here](assessment.md).\n\nMy project organization notes are [here](proj_org_notes.md).\nYour editor should look like this.\n\nHere’s what the special characters do:\n\n# creates a header\n*text* puts text in italics\n[text](url) creates a link\n\nYou’ll create assessment.md and proj_org_notes.md in the assessment.\nLet’s see it in action. First, activate Pages on GitHub. go to your GitHub repo in your browser. Click on Settings and choose Pages under Code and automation. Under Source it should say Deploy from a branch. Under Branch, change None to main and the directory from / (root) to /docs. Click Save.\nNow you need to give GitHub something to deploy. Go back to RStudio and commit all your new and changed files. To do this, go to the Git pane. You’ll see a list of new and modified files. Check the boxes next to all of them to stage the files. This tells git you’d like to commit them. Click on the Commit button to launch the Review Changes dialog and add a commit message. As a rule of thumb, keep commit messages short (&lt;50 characters). Click the Commit button, then click the Push button.\nIt will take GitHub a minute or two to render your site. Return to your GitHub repo in your browser and switch to the Actions tab. You’ll see a job running called pages build and deployment with a yellow dot next to it. Click on the job and wait for the steps to all turn green.\n\nClick on the link under deploy. Notice the url, it’s [yourgithubname].github.io/[yourreponame]. That’s the pattern GitHub pages uses. You should see your README rendered in all its glory. Call the instructor over for debugging if necessary followed by a high five.\n\n\n\nAssessment\n\nReading questions\nAnswer the following questions about the reading. Create a markdown file called docs/reading.md. Put your answers to the following questions in that file.\n\nProject workflows\nBryan (2017)\n\nWhat problems can setwd() cause in your scripts and how do RStudio projects address them?\nWhen you call rm(list=ls()), what is removed from your environment? What’s left over that restarting your R session would remove? What’s the keyboard shortcut for restarting your R session?\n\n\n\nVersion control\nYou either read Bryan (2018) or Braga et al. (2023). Answer the questions for the paper you read.\nBryan (2018)\n\nThe basic git commands are commit, push, and pull. Which commands change happen locally (i.e., on your computer)? Which happen remotely?\nWhy do diffs work for source code (e.g., .R files) but not Word documents (i.e., .docx files)?\nWhy is Markdown useful for GitHub repos?\n\nBraga et al. (2023)\n\nImagine you’re working with a few collaborators on an analysis. Come up with two examples of Issues you might open. How would using Issues differ from communicating over email?\nWhat are three ways GitHub features can promote open science practices?\n\n\n\n\nProject organization notes\nYou were (probably) exposed to a lot of new material in this lesson. In this part of the assessment, you’ll create a cheat sheet for yourself. Create a markdown file called docs/proj_org_notes.md. In that file, create a short document you can use to guide you when you create your next analysis project. Consider which steps only need to be configured once versus which have to be repeated for every project. If there were steps you found counterintuitive or confusing today, make a note of why you did them or how you figured them out.\n\n\nSubmission\nSubmit your assessment by emailing your GitHub pages url to mczapans [at] ucsc [dot] edu7."
  },
  {
    "objectID": "lessons/01_comp_proj_org.html#footnotes",
    "href": "lessons/01_comp_proj_org.html#footnotes",
    "title": "Week 1 - Computational project organization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you can’t find the Terminal pane, ask your instructor for help. Sometimes it needs to be activated the first time you use it.↩︎\nThe internet is full of vandals. C’est la vie.↩︎\nPATs are kind of like your git configuration in that they’re “set it and forget it” commands. You will only need to update your PAT every three months if you set Expiration to 90 days. The rest of the time you don’t need to think about it. And if you’re wondering why we set PATs to expire at all, see the previous footnote about vandals.↩︎\nWe won’t cover branches in this course, but they’re an important part of version control. Suffice to say, every repo has a main branch which by default is called “master”. That’s racist terminology, so there’s a concerted effort to use “main” instead.↩︎\n“Quarto, what’s that?” you might say. Quarto documents combine text, code, figures, and tables. They’re extremely useful for scientific analyses and writing! You’ll learn more about them next week. If you’re familiar with RMarkdown, Quarto is its next evolutionary step.↩︎\nIf you hold the rights to your data and they’re safe to put online (i.e., nothing sensitive), I strongly encourage you to make them public from the start. You might worry about getting scooped, which is understandable. But in my experience publishing datasets, it’s incredibly difficult to get other scientists to look at your data when you’re literally advertising them. In my opinion, the advantages of making data public far outweigh the risks. It facilitates collaboration and makes it easier to publish your data when you wrap up your project.↩︎\nTBH I haven’t figured out yet how to access UCSC’s Canvas as an instructor. You’ll submit future assessments through Canvas once I figure that out.↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Data Science for Eco/Evo\nBIOE215, 3 Credits, Fall 2023"
  },
  {
    "objectID": "syllabus.html#course",
    "href": "syllabus.html#course",
    "title": "Syllabus",
    "section": "",
    "text": "Data Science for Eco/Evo\nBIOE215, 3 Credits, Fall 2023"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Syllabus",
    "section": "Instructor",
    "text": "Instructor\nDr. Max Czapanskiy (he/they)\nEmail: mczapans@ucsc.edu"
  },
  {
    "objectID": "syllabus.html#location",
    "href": "syllabus.html#location",
    "title": "Syllabus",
    "section": "Location",
    "text": "Location\nCoastBio 115"
  },
  {
    "objectID": "syllabus.html#times",
    "href": "syllabus.html#times",
    "title": "Syllabus",
    "section": "Times",
    "text": "Times\nMondays, 3:30-5:30 pm"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office Hours",
    "text": "Office Hours\nTimes: Thursdays 3:30-5:30 pm\nLocation: CoastBio Otter Conference Room\nOr by appointment."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\nThis course will aim to be practical and provide structure for learning the computational skills most eco/evo grad students have to pick up on their own. Topics will include programming best practices in R, organizing data and computational projects, and planning for reproducibility. This is a chance to gain knowledge and experience with the nuts and bolts of making your science work in code, without trying to learn generalized linear models at the same time. The course structure will be self-directed for weeks 1-6, followed by a final project in weeks 7-10."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nWeek 1: Computational project organization\nWeek 2: Computational thinking I\nWeek 3: Working with data\nWeek 4: Computational thinking II\nWeek 5: Troubleshooting\nWeek 6: Wrap up self-directed portion of class\nWeeks 7-9: Final projects\nWeek 10: Communities of practice"
  },
  {
    "objectID": "syllabus.html#how-class-works",
    "href": "syllabus.html#how-class-works",
    "title": "Syllabus",
    "section": "How class works",
    "text": "How class works\nThe class format is self-paced with an emphasis on small-group learning. Each class will begin with a short discussion on the assigned readings, then we’ll break out into groups. I will require everyone to complete some lessons start to finish (for example, the project organization lesson). Other lessons will let you jump in at the point appropriate to your level of experience.\nAfter week 6, class time will be devoted to your final projects. These will be replication studies. You and your group will find a paper that you want to replicate some part of, either a figure or a table. Using the skills you’ve learned in this class, you will create a stand-alone analysis that replicates their work. I will encourage all groups to submit their replication to an appropriate journal such as Re-Science C.\nThis class covers computational thinking, programming in base R and the tidyverse, project organization and management, and open science practices. You all probably have very different levels of preparation in these areas. For example, if you have more base R programming experience, I’ll suggest you tackle advanced lessons like functional programming. If you feel your foundation is a little patchy, I’ll suggest you deepen your understanding of the core data structures. This means most students will complete somewhat distinct subsets of the available material. That’s by design! You’re not expected to do the same lessons as everyone else. You are expected to do the lessons that best support your growth as a scientist. Speaking of expectations…"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\nThis course offers more material than can be covered in the time allocated and students should not expect to complete all of it! Biology grad students have diverse backgrounds, training, and experience with computational skills. Some of the available material will be too basic or too advanced for you at this time. With help from the instructor, you will identify what content will contribute most to your academic and professional success.\n\nWhat you can expect from your instructor\nThe instructor is here to support your learning and provide you with guidance as you develop your understanding of and skills in data science. I strive for an inclusive and collaborative classroom and welcome any suggestions for improvement. I will do my best to provide useful content, rapid feedback, and enthusiastic support, so let me know if I can do anything more. I highly encourage everyone to visit me in office hours or to set up a meeting, even if you don’t feel that you have questions. I want to get to know you and support you in this learning experience!\n\n\nWhat your instructor expects from you\nI expect you to attend class regularly, complete the self-guided material at the pace that works best for you, and contribute to your group’s final project. Your learning depends on your own preparation and engagement as well as that of your fellow students. I encourage you to rely on each other, as well as the instructor, for problem solving and finding solutions. Teaching reinforces learning, so after you complete lessons be prepared to assist other students with that material. Each member of this class has different ideas and perspectives that will enrich the experience for everyone else, so I ask you to be respectful and thoughtful in your interactions. To get the most out of the class, you should be prepared to share your ideas, ask questions, listen actively and collaborate effectively during small group work. Never hesitate to email your instructors, stop by office hours, or set up a meeting. This class should challenge you, but I believe everyone has the ability to succeed with some effort."
  },
  {
    "objectID": "syllabus.html#why-isnt-there-a-remote-option",
    "href": "syllabus.html#why-isnt-there-a-remote-option",
    "title": "Syllabus",
    "section": "Why isn’t there a remote option?",
    "text": "Why isn’t there a remote option?\nRemote options have become standard for most classes and I wish I could offer one for this course. However, the structure of the class does not lend itself to hybrid formats because of the emphasis on peer learning and 1-on-1 coaching from the instructor. In the future, I hope to offer online and in-person versions of the course. For now, I only have resources to support the in-person version.\nFor the same reason, I can’t support auditing the class at this time. However, I am happy to share the educational materials with anyone who’s interested. I will try to connect these students so you can form a study group. If that happens, I will add additional office hours to meet with you."
  }
]